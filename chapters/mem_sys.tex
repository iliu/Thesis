While pipelines designs continue to improve, memory technology has been struggling to keep up with the increase in clock speed and performance.
Even though memory bandwidth can be improved with more bank parallelization, the memory latency remains the bottle neck to really improving memory performance.
Common memory technologies used in embedded systems contain a significant trade off between access latency and capacity. 
Static Random-Access Memories (SRAM) provides sufficient latency that allows single cycle memory access latencies from the pipeline.
However, the hardware cost to implement each memory cell prohibits large capacities to be implemented close to the processor.
On the other hand, Dynamic Random-Access Memories (DRAM) uses a compact memory cell design that can easily be designed into larger capacity memory blocks.
But the memory cell of DRAMs must be constantly refreshed due to charge leakage, and the large capacity of DRAM cells in a memory block design often prohibit faster access latencies.
%TODO: talk about about embedded systems seldom need disk?
To bridge the latency gap between the pipeline and memory, smaller memories are placed in between the pipeline and larger memories to act as a buffer, forming a memory hierarchy.
The smaller memories give faster access latencies at the cost of lower capacity, while larger memories make up for that with larger capacity but slower access latencies. 
The goal is to speed up program performance by placing commonly accessed values closer to the pipeline, while less access values are placed farther away.

\subsection{Memory Hierarchy}
\subsubsection{Caches}
\begin{wrapfigure}{r}{0.5\textwidth}
  \vspace{-20pt}
  \begin{center}
    \includegraphics[scale=.5]{figs/conventional_mem_hierarchy}
  \end{center}
  \vspace{-20pt}
  \caption{Memory Hierarchy w/ Caches}
  \label{fig:conventional_mem_hierarchy}
  \vspace{-10pt}
\end{wrapfigure}   
A \emph{CPU Cache} (or cache) is commonly used in the memory hierarchy to manage the smaller fast access memory made of SRAMs.
The cache manages of contents of the fast access memory in hardware by leveraging the spatial and temporal locality of data accesses. 
The main benefits of the cache is that it abstracts away the memory hierarchy from the programmer.
When a cache is used, all memory accesses is routed through the cache. 
If the data from the memory access is on the cache, then a cache hit occurs, and the data is returned right away.
However, if data is not on the cache, then a cache miss occurs, and the cache controller fetches the data from the larger memory and adjusts the memory contents on the cache. 
The replacement policy of the cache is used to determine which cache line, the unit of memory replacement on caches, to replace. 
A variety of cache replacement policies have been researched~\todo{cite} and used to optimize for memory access patterns of different applications. 
In fact, modern memory hierarchies often contain multiple layers of hierarchy to balance the trade-off between speed and capacity.
A commonly used memory hierarchy is shown in figure~\ref{fig:conventional_mem_hierarchy}.
If the data value is not found in the L1 cache, then it is searched for in the L2 cache. 
If the L2 cache also misses, then the data is retrieved from main memory, and sent back to the CPU while the L1 and L2 cache updates its contents.
Often times different replacement policies are used at different levels of the memory hierarchy to optimize the hit rate or miss latency of the memory access.
Benchmarks have show that caches can have hit rates up to \todo{find number}\% \todo{and cite}. 

As sophisticated as this seems, the program is oblivious to the different levels of memory hierarchy, and whether or not an access hits the cache or goes all the way out to main memory. 
The memory hierarchy is abstracted away from the program, and the cache gives its best-effort to optimize memory access latencies.
This is one of the main reasons for the cache's popularity; the programmer does not need to put in extra effort to get a reasonable amount of performance.
A program can be run on any memory hierarchy configuration without modification and still obtain reasonable performance from the hardware.   
Thus, for general purpose applications, caches give the ability to improve design times and decrease design effort.
However, the cache makes no guarantees on actual memory access latencies and program performance. 
The execution time of programs could highly vary depending on a number different factors, cold starts, the execution context previously running, interrupt routines, and even branch mispredictions that cause unnecessary cache line replacements.  
Thus, when execution time is important, the variability and uncontrollability of caches may outweigh the benefits it provides. 

The cache uses internal states stored in hardware to manage the replacement of contents on it. 
As the programmer cannot control the states of the cache explicitly, it is extremely difficult to analyze the execution time of a program running with caches.
At an arbitrary point in the program, the state of the cache is unknown to the software.
Whether a memory access hits or misses the cache cannot to determined easily, so the conservative worst-case execution time analysis well need to assume the worst case as if the memory access was directly to main memory.
In theory, \todo{Cite and summarize Jan's thesis on cache analysis} showed that for certain cache replacement policies, it is possible to obtain tighter execution time analysis. 
However, the complexity of modern memory hierarchies with caches make timing analysis extremely difficult, and introduce high variability in program execution time.

Even outside of real-time applications, caches present side effects that it were not intended for.  
For applications that require extremely high speed, the best-effort memory management that caches offer simply is not good enough.
In those situations, programs need to be hand tuned and tailored to specific cache architectures and parameters.
Algorithm designers tune the performance of algorithms by reaching beneath the abstracted away memory architecture to enforce data access patterns to conform to the cache replacement policies and cache line sizes. 
Blocking~\cite{Lam91thecache}, for example, is a well-known technique to optimize algorithms for high performance.
Instead of operating on entire rows or columns of an array, algorithms are rewritten to operate on a subset of the data at a time, or blocks, so the faster memory in the hierarchy can be reused.
\todo{talk about LAPACK? Libraries that tune programs to caching}.
In this case, we see that the hidden memory hierarchy actually could degrade program performance.    
Multithreaded threaded architectures with shared caches amongst the hardware threads can suffer from \emph{cache thrashing}, an effect where different threads' memory accesses evict the cached lines of others.
In this situation, it is impossible for hardware threads have any knowledge on the state of the cache, because it is simultaneously being modified by other threads in the system. 
As a result, the hardware threads have no control over which level in the memory hierarchy they are accessing, and the performance highly varies depending on what is running on other hardware threads. 
For multicore architectures, caches create a data coherency problem when keeping data consistent between the multiple cores.
When the multiple cores are sharing memory, each core's private cache may cache the same memory address. 
If one core writes to a memory location that is cached in its private cache, then the other core's cache would contain stale data. 
Various methods such as bus snooping\todo{cite} or implementing a directory protocol\todo{cite} have been proposed to keep the data consistent in all caches. 
However, doing this scalably and efficiently is still a hot topic of research today\todo{cite all work on cache coherency}.

\subsubsection{Scratchpads}
We cannot argue against the need for a memory hierarchy, as there is an undeniable gap between processor and DRAM latency.
However, instead of abstracting away the memory hierarchy from the programmer, we propose to expose the memory layout to the software.  
\emph{Scratchpads} were initially proposed for their power saving benefits over caches\todo{cite}.
\todo{Papers that show scratchpads were already popular? Mention that they are already used. The Cell processor, Nvidia's 8800 GPU}
Scratchpads uses the same memory technology as caches, but does not implement the hardware controller to manage its memory contents.
Scratchpads have reduced access latency, area and power consumption compared to caches. 
Memory operations that access the scratchpad region take only a single cycle to complete, which is the same as a cache hit.
Thus, scratchpads may serve as the fast-access memory in a memory hierarchy, instead of caches. 
Unlike caches that overlay address space with main memory, scratchpads occupy a distinct address space in memory, so it does not need to check whether the data is on the scratchpad or not. 
Furthermore, the memory access time of each memory request is only depend on the memory address it is accessing. 
This drastically improves the predictability of memory access times, and reduces the variability of execution time introduced with caches.  
By using scratchpads, we explicitly expose the memory hierarchy to software, giving the programmer full control over the management of memory contents within the memory hierarchy.
\todo{show figure of memory hierarchy with scratchpads}

Two allocation schemes are commonly employed to manage the contents of scratchpads in software.
Static allocation schemes allocate data on the scratchpad during compile time, and the contents allocated on the scratchpad does not change throughout program execution. 
Static scratchpad allocation schemes~\cite{Suhendra2005WCETSPM, Patel2008PRETSPM} often use heuristics or compiler-based static analysis\todo{citation} of program to find the most commonly executed instructions or data structures, and allocate them statically on the scratchpad to improve program performance. 
Dynamic allocation schemes modify the data on the scratchpad during run time in software through DMA mechanisms.
The allocation could either be automatically generated and inserted by the compiler, or explicitly specified by the user programmatically.
Embedded system designs typically deal with limited resources and other design constraints, such as less memory or hard timing deadlines.    
Thus, the design of these systems often contain analysis on memory usage etc to ensure that the constraints are met. 
Actor oriented models of computations, such as Dataflow\todo{cite} or Giotto\todo{cite}, allow users to design systems at a higher level.
These higher level actor oriented programming models exposes the structure and semantics of the model for better analysis, which can be used to optimize scratchpad allocation dynamically.
Bandyopadhyay~\cite{Bandyopadhyay06_AutomatedMemoryAllocationOfActorCodeDataBufferInHeterochronous} presented an automated memory allocation of scratchpads for the execution of Heterochronous Dataflow models.
The Heterochronous Dataflow (HDF) model is an extension to the Synchronous Dataflow (SDF) model with finite state machines (FCM). 
The HDF models contain different program states, each state executing a SDF model that contains actors communicating with each other. 
Bandyopadhyay analyzed the actor code and the data that was being communicated in each HDF state.
The dynamic scratchpad allocation is inserted during state transitions, and the memory allocated is optimized for each HDF state. 
This allocation not only showed roughly 17\% performance improvement compared to executions using LRU caches, but also more predictable program performance.

The underlying memory technology that is used to make both scratchpads and caches is not inherently unpredictable, as SRAMs provide constant low-latency access time. 
However, caches manage the contents of the SRAM in hardware. 
By using caches in the memory hierarchy, the hierarchy is hidden from the programmer, and hardware managed memory contents creates highly variable execution times with unpredictable access latencies. 
Scratchpads on the other hand exposes the memory hierarchy to the programmer, allowing more predictable and repeatable memory access performances.
Although the allocation of scratchpads could be challenging, but it also provides opportunity for high efficiency, as it can be tailored to specific applications.   

\subsection{DRAM Memory Controller}
Because of its high capacity, DRAMs are often employed in modern embedded systems to cope with the increasing code and data sizes.  
However, bank conflicts and refreshes within the DRAM can cause memory accesses to stall, further increasing the memory latency. 
Modern memory controllers are designed to optimize average-case performance by queueing and reordering memory requests to improve throughput of memory requests. 
This results in unpredictable and varying access times and increased worst-case access time for each memory request.
In this section we will present a DRAM memory controller that privatizes DRAM banks with scheduled memory refreshes to provide improved worst-case latency and predictable access time.    
The contributions from this section is research done jointly with several co-authors from Reineke et. al~\cite{ReinekeLiuPatelKimLee11_PRETDRAMControllerBankPrivatizationForPredictability}. 
We do not claim sole credit for this work, and the summary is included in this thesis only for completeness purposes. 
We will first give some basic background on DRAM memories, then present the predictable DRAM controller designed.

\subsubsection{DRAM Basics}

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{figs/dram-overview.pdf}
\vspace{-13mm}
\caption{A dual-ranked dual in-line memory module.}\label{fig:dram_basics}
\vspace{-6mm}
\end{center}
\end{figure} 

Figure~\ref{fig:dram_basics} shows the structure of a dual ranked in-line DDRII DRAM module.
Starting from the left, a basic \textbf{\emph{DRAM cell}} consists of a capacitor and a transistor. 
The capacitor charge determines the value of the bit, which can be accessed by triggering the transistor. 
Because the capacitor leaks charge, it must be refreshed periodically, typically every 64 ms or less~\cite{jedec}.
A \textbf{\emph{DRAM array}} is made of a two-dimensional array of DRAM cells.
Each access is made to the DRAM array goes through two phases: a row access followed by one or more column accesses.   
During the row access, one of the rows in the DRAM array is moved into the row buffer.
To read the value in the row buffer, the capacitance of the DRAM cells is compared to the wires connecting them with the row buffer.
The wires need to be precharged closed to the voltage threshold so the sense amplifiers can detect the bit value.
Columns can be read and written to quickly after the row is in the row buffer. 
The \textbf{\emph{DRAM device}} consists of banks formed of DRAM arrays. 
Modern DRAM devices have multiple banks, control logic, and I/O mechanisms to read from and write to the data bus as shown in the center of \ref{fig:dram_basics}.
Banks can be accessed concurrently, but the data, command and address busses are shared within the device, which is what the memory controller uses to send commands to the DRAM device. 
The following table\footnotemark lists the four most important commands and their function:
\footnotetext{This table is as shown in ~\cite{ReinekeLiuPatelKimLee11_PRETDRAMControllerBankPrivatizationForPredictability}}
%\begin{table}
\begin{center}
\begin{smalltabular}{p{13mm}p{6mm}p{10cm}}
Command 				& Abbr. & Description\\\hline
Precharge    			& PRE   & Stores back the contents of the row buffer into the DRAM array, and prepares the sense amplifiers for the next row access.\\
Row\hspace{13mm} access & RAS	& Moves a row from the DRAM array through the sense amplifiers into the row buffer.\\
Column access 			& CAS   & Overwrites a column in the row buffer or reads a column from the row buffer.\\
Refresh					& REF	& Refreshes several\footnotemark rows of the DRAM array. This uses the internal refresh counter to determine which rows to refresh.\\
\end{smalltabular}
%\caption{Overview of DDR2 Commands~\cite{ReinekeLiuPatelKimLee11_PRETDRAMControllerBankPrivatizationForPredictability}}
\label{tab:ddr2-commands}
\end{center}
%\end{table}
\footnotetext{The number of rows depends on the capacity of the device.}

To perform reads or writes, the controller first sends the PRE command to precharge the bank containing the data. 
Then, a RAS is issued to select the row, and one or more CAS commands can be used to access the columns within the row. 
Accessing columns from the same row does not require additional PRE and RAS commands, thus higher throughput can be achieved by performing column accesses in burst lengths of four to eight words.  
Column accesses can immediately be followed by a PRE command to decrease latency when accessing different rows. 
This is known as auto-precharge (or closed-page policy).
Refreshing of the cells can be done in two ways.
First, by issuing a refresh command, which refreshes all banks of the device simultaneously. 
The refresh latency depends on the capacity of the device, but the DRAM device manages a counter to step through all the rows.
The rows on the device could also be manually refreshed by performing row accesses to them.
Thus, the memory controller could performance row accesses on every row within the 64 ms refresh period.
This requires the memory controller to keep track of the refresh status of the device and issue more refresh commands, but each refresh takes less time because it is only a row access. 
\textbf{\emph{DRAM modules}} are made of several DRAM devices integrated together for higher bandwidth and capacity. 
A high-level view of the dual-ranked dual in-line memory module (DIMM) is shown in the right side of \ref{fig:dram_basics}.
The DIMM has eight DRAM devices that are organized in two ranks, which share the address, command and data bus. 
The two ranks share the address, command inputs, and the 64-bit data bus.
The chip is used to determine which ranks is addressed.
All devices within a rank are accessed simultaneously when the rank is addressed, and the results are combined to form the request response.  
% Due to the sharing of I/O mechanisms within a device, consecutive accesses to the same rank are more constrained than consecutive accesses to different ranks, which only share the command and address as well as the data bus.
% We later exploit this subtle difference by restricting consecutive accesses to different ranks to achieve more predictable access latencies.  
% We explain this in more detail in \ref{sec:pret_dram_controller}. 
Our controller makes use of a feature from the DDR2 standard known as posted-CAS.  
Unlike DDR or other previous versions of DRAMs, DDR2 can delay the execution of CAS commands (posted-CAS) for a user-defined latency, known as the additive latency ($AL$). 
Posted-CAS can be used to resolve command bus contention by sending the posted-CAS earlier than the corresponding CAS needs to be executed.

Table~\ref{table:ddr2-constraints} gives an overview of timing parameters for a DDR2-400 memory module.
These timing constraints come from the internal structure of DRAM modules and DRAM cells.
For example, $t_{RCD}, t_{RP}$, and $t_{RFC}$ are from the structure of DRAM banks that are accessed through sense amplifiers that need to be precharged.
$t_{CL}, t_{WR}, t_{WTR}$, and $t_{WL}$ result from the structure of DRAM banks and DRAM devices.
The four-bank activation window constraint $t_{FAW}$ constrains rapid activation of multiple banks which would result in too high a current draw.
The memory controller must conform to these timing constraints when sending commands to the DDR2 module.  
%The additive latency, $t_{AL}$, can be set by the user and determines how many cycles after a posted-CAS a CAS is executed.
Here we only gave a quick overview of DRAMs, we refer more interested readers to Jacob et al.~\cite{JaNgWa07} for more details.

\begin{table}
\begin{center}
\begin{smalltabular}{p{0.8cm}p{2.0cm}p{11cm}}
Para-meter	& Value (in cycles at 200 MHz)	& Description \\\hline
$t_{RCD}$			& 3						& Row-to-Column delay: time from row activation to first read or write to a column within that row.\\
$t_{CL}$			& 3						& Column latency: time between a column access command and the start of data being returned.\\
$t_{WL}$			& $t_{CL}-1=2$			& Write latency: time after write command until first data is available on the bus.\\
$t_{WR}$			& 3						& Write recovery time: time between the end of a write data burst and the start of a precharge command.\\
%$t_{CCD}$			& $\burstlength/2$ 				& CAS to CAS command delay. Minimum time between two read commands or two write commands.\\
$t_{WTR}$ 			& 2 					& Write to read time: time between the end of a write data burst and the start of a column-read command.\\% Allows the sense amplifiers to restore the data in the DRAM array.\\
$t_{RP}$			& 3						& Time to precharge the DRAM array before next row activation.  \\
%$t_{RTRS}$			& 1\todo{check this}	& Rank-to-rank switching time.\\ 
$t_{RFC}$			& 21					& Refresh cycle time: time interval between a refresh command and a row activation.\\
%$t_{REFI}$			& 1560    				& Refresh to refresh interval \\
%$t_{RAS}$ 			& $t_{RCD}+t_{WL}+t_{WR} = 8$ & Minimum time after an activate command to a bank until that bank is allowed to be precharged.\\
%$t_{RC}$			& $t_{RAS}+t_{RP}=11$	& Row cycle time: minimum time between successive activate commands to the same bank.\\
%$t_{RTP}$			&   			& Minimum time between a precharge command on a bank and a successive activate command.\\
$t_{FAW}$			& 10					& Four-bank activation window: interval in which maximally four banks may be activated.\\
$t_{AL}$			& set by user			& Additive latency: determines how long posted column accesses are delayed.
\end{smalltabular}
\vspace{-7.5mm}
%\todo{group constraints by their origin: constraints due to DRAM array structure, constraints due to sharing within DRAM device, constraints due to sharing of bus among ranks}
\end{center}
\caption{\small{Overview of DDR2-400 timing parameters of the Qimonda HYS64T64020EM-2.5-B2.~\cite{ReinekeLiuPatelKimLee11_PRETDRAMControllerBankPrivatizationForPredictability}}}\label{table:ddr2-constraints}
\end{table}

\subsubsection{Predictable DRAM Controller}
\label{sec:pret_dram_controller}
We will split the discussion of the predictable DRAM controller into its backend and frontend. 
The backend translates memory requests into DRAM commands that are sent to the DRAM module.
The frontend manages the interface to the pipeline along with the responsibility of scheduling the refreshes.
Here we specifically refer to a DDR2 667MHz/PC2-5300 memory module operating at 200Mhz, which has a total size of 512MB over two ranks with four banks on each rank.
While our discussion of the design of this DRAM controller is specific to our DDR2 memory module, the key design features are applicable to other modern memory modules.

\paragraph{Backend}
Conventional DRAM memory controllers view the entire memory device as one resource, and any memory requests can access the whole DRAM device. 
Subsequent memory accesses can target the same bank within the DRAM, which results in the need for memory requests to be queued and serviced sequentially, without exploiting bank parallelism.
Our controller views the memory devices as independent resource partitioned by banks. 
Specifically, we partition our memory module into four resources, each consisting of two banks within the same rank. 
The banks within each partition can be arbitrarily chosen, but all banks within a resource must belong to the same rank, and each of the ranks must contain at least two resources.
This is to avert access patterns that would incur high latency from the sharing of resources within banks and ranks.
The partitioning of the memory device allows us to fully exploit bank parallelism by accessing the resources in a periodic and pipelined fashion.
The periodic access scheme to the four resources interleaves each memory access between the ranks.
Subsequent accesses to the same rank go to different resources grouped from banks.  
Figure~\ref{fig:backend} shows an example of the following access requests from the frontend: read from resource 0 in rank 0, write to resource 1 in rank 1, and read from resource 2 in rank 0. 

\begin{figure}
\begin{center}
\includegraphics[width=0.92\linewidth]{figs/backend}
\end{center} 
\caption{\small{The periodic and pipelined access scheme employed by the backend~\cite{ReinekeLiuPatelKimLee11_PRETDRAMControllerBankPrivatizationForPredictability}.}}\label{fig:backend}
\end{figure}

Each access request is translated into a RAS (Row Access), posted-CAS (Column Access) and NOP command, which we call an access slot. 
The NOP command in the access slot is inserted between any two consecutive requests to avoid a collision on the data bus that occurs when a read request follows and a write request.
This collision is cause by the one cycle offset between the read and write latencies. 
The RAS command moves a row into the row buffer, and the CAS command accesses the columns within the row loaded into the row buffer. 
CAS commands can be either reads or writes, causing a burst transfer of $8 \cdot 4 = 32$ bytes that occupies the data bus for two cycles (as two transfers occur in every cycle).
We send a posted-CAS instead of a normal CAS in order to meet the row to column latency shown in \ref{table:ddr2-constraints}.
This latency specifies that the RAS command and the first CAS command need to be 3 cycles apart.  
However, figure~\ref{fig:backend} shows that manually issuing a CAS command to the first resource 3 cycles after its RAS command would cause a command bus conflict with the RAS command for the second resource.
Thus, we instead set the additive latency $t_{AL}$ to 2 and use the posted-CAS that offsets the CAS command to conform to the row to column latency.
This allows our memory controller to preserve our pipelined access scheme while meeting the latency requirements of the DRAM.   
We use a closed-page policy (also known as auto-precharge policy), which causes the accessed row to be immediately precharged after performing the column access (CAS), preparing it for the next row access.
If there are no requests for a resource, the backend does not send any commands to the memory module, as is the case for resource 3 in \ref{fig:backend}.

Our memory design conforms to all the timing constraints listed in table~\ref{table:ddr2-constraints}.
The write-to-read timing constraint $t_{WTR}$, incurred by the sharing of I/O gating within ranks, is satisfied by alternating accesses between ranks. 
The four-bank activation window constraint is satisfied because within any window of size $t_{FAW}$ we activate at most four banks within the periodic access scheme. 
Write requests with the closed-page policy requires 13 cycles to access the row, perform a burst access, and precharge the bank to prepare for the next row access.
However, our periodic access scheme has a period of 12 cycles, as each access slot is 3 cycles, and there are four resources accessed. 
Thus, a NOP is inserted after the four access slots: to increase the distance between two access slots belonging to the same resource from 12 to 13 cycles.
As a result, the controller periodically provides access to the four resources every 13 cycles.
The backend does not issue any refresh commands to the memory module.
Instead, it relies on the frontend to refresh the DRAM cells using regular row accesses.

% \paragraph{Longer Bursts for Improved Bandwidth}
% Depending on the application, bandwidth might be more important than latency.
% Bandwidth can be improved by increasing the burst length from 4 to 8.
% Extending the proposed access scheme to a burst length of 8 is straightforward with the insertion of two additional NOP commands after each request to account for the extra two cycles of data being transfered on the data bus.  
% In this case, the access slot latency for each request is increased from three to five to include the extra two NOP commands, and data will be transferred in four out of five cycles rather than in two out of three.
% Then, of course, latency of transfers of size less than or equal to 32 bytes increases, but the latency of large transfers decreases and higher bandwidth is achieved. 

\begin{wrapfigure}{r}{0.5\textwidth}
\begin{center}
\includegraphics[width=1.1\linewidth]{figs/dram-backend-implementation}
\end{center}
\mycaption{\small{Sketch of implementation of the backend~\cite{ReinekeLiuPatelKimLee11_PRETDRAMControllerBankPrivatizationForPredictability}.}}\label{fig:dram-backend-implementation}
\end{wrapfigure}

A high level block view of our backend implementation is shown in figure~\ref{fig:dram-backend-implementation}.
Each resource has a single request buffer and a respond buffer.
These buffers are used to interface with the frontend.   
A request is made of an access type (read or write), a logical address, the data to be written for write requests. 
Requests are serviced at the granularity of bursts, i.e. 32 bytes in case of burst length 4 and 64 bytes in case of burst length 8.
A modulo-13 counter is used to implement the 13 cycle periodic access scheme in our controller.   
The ``resource'' and ``command" blocks are combinational circuits that are used to select the correct request buffer and generate the DRAM commands to be sent out. 
The ``memory map" block is where logical addresses are mapped to physical addresses that determine the rank, bank, row and column to access.
The data for read requests are latched into the response buffers to be read by the frontend.  

\paragraph{Frontend}
The frontend of our memory controller integrates with various architectures and manages the refresh of the DRAM device.
The privatization of DRAM banks    

%For us, the main application of the backend is within the PTARM PRET architecture~\cite{Liu10}.


In this section, we discuss our integration of the backend within the PTARM PRET architecture~\cite{Liu10}.
We also  discuss how the PRET DRAM controller could be integrated into other predictable architectures, such as those proposed by the MERASA~\cite{Ungerer10}, PREDATOR~\cite{Wilhelm09}, JOP~\cite{Schoeberl2008265}, or CoMPSoC~\cite{Hansson09} projects, which require predictable and composable memory performance.

% \subsubsection{Integration with the PTARM Architecture}
% 
% PTARM~\cite{Liu10}, a PRET machine~\cite{Edwards:2007:pret}, is a thread-interleaved implementation of the ARM instruction set.
% Thread-interleaved processors preserve the benefit of a multi-threaded architecture -- increased throughput, but use a predictable fine-grained thread-scheduling policy -- round robin.
% If there is the same number of hardware threads as there are pipeline stages, at any point in time, each stage of the pipeline is occupied by a different hardware thread; there are no dependencies between pipeline stages, and the execution time of each hardware thread is independent of all others. PTARM has four pipeline stages and four hardware threads.
% Each hardware thread has access to an instruction scratchpad and a data scratchpad. 
% The scratchpads provide single-cycle access latencies to the threads.
% The two scratchpads are shared among the four threads, allowing for shared memory communication among the threads.

However, due to the thread-interleaving, only one thread can access the scratchpad at any time.
Each hardware thread is also equipped with a direct memory access (DMA) unit, which can perform bulk transfers between the two scratchpads and the DRAM.
Both scratchpads are dual-ported, allowing a DMA unit to access the scratchpads in the same cycles as its corresponding hardware thread.
In our implementation of thread-interleaving, if one thread is stalled waiting for a memory access, the other threads are unaffected and continue to execute normally.

The four resources provided by the backend are a perfect match for the four hardware threads in the PTARM thread-interleaved pipeline.
We assign exclusive access to one of the four resources to each thread.
In contrast to conventional memory architectures, in which the processor interacts with DRAM only by filling and writing back cache lines, there are two ways the threads can interact with the DRAM in our design.
First, threads can initiate DMA transfers to transfer bulk data to and from the scratchpad. 
Second, since the scratchpad and DRAM are assigned distinct memory regions, threads can also directly access the DRAM through load and store instructions. 

%\HP{Why not introduce figure 4 here?}
%\IL{Why introduce figure 4 here?}
%\myparagraph{Connection of the Controller Backend to PTARM}
Whenever a thread initiates a DMA transfer, it passes access to the DRAM to its DMA unit, which returns access once it has finished the transfer. 
During the time of the transfer, the thread can continue processing and accessing the two scratchpads.
If at any point the thread tries to access the DRAM, it will be blocked until the DMA transfer has been completed.
Similarly, accesses to the region of the scratchpad which are being transferred from or to will stall the hardware thread\footnote{This does not affect the execution of any of the other hardware threads.}.
\ref{fig:pretintegration} shows a block diagram of PTARM including the PRET DRAM controller backend and the memory module.
The purpose of the frontend is to route requests to the right request buffer in the backend and to insert a sufficient amount of refresh commands, which we will discuss in more detail.

\begin{figure}
\begin{center}
\includegraphics[width=0.7\linewidth]{figs/pret-integration}
\end{center}
\caption{Integration of PTARM core with DMA units, PRET memory controller and dual-ranked DIMM~\cite{ReinekeLiuPatelKimLee11_PRETDRAMControllerBankPrivatizationForPredictability}.}\label{fig:pretintegration} % \todo{modify to show four independent resources in backend}
\end{figure}

%\HP{What is this next paragraph trying to discuss?  I'd rework this paragraph to say that we present an example read access and discuss the interaction of the pipeline and the controller}
When threads directly access the DRAM through load (read) and store (write) instructions, the memory requests are issued directly from the pipeline. 
\ref{fig:readlatencyderivation}, which we will later use to derive the read latency, illustrates the stages of the execution of a read instruction in the pipeline.
At the end of the memory stage, a request is put into the request buffer of the backend.
Depending on the alignment of the pipeline and the backend, it takes a varying number of cycles until the backend generates corresponding commands to be sent to the DRAM module.
After the read has been performed by the DRAM and has been put into the response buffer, again, depending on the alignment of the pipeline and the backend, it takes a varying number of cycles for the pipeline to reach the write-back stage of the corresponding hardware thread.
Unlike the thread-interleaved pipeline, the DMA units are not pipelined, which implies that there are no ``alignment losses'':
the DMA units can fully utilize the bandwidth provided by the backend.


\paragraph{Store Buffer}
%\HP{Is this PRET-like?  Seems very unpret to me?}
Stores are fundamentally different from loads in that a hardware thread does not have to wait until the store has been performed in memory.
By adding a single-place store buffer to the frontend, we can usually hide the store latency from the pipeline.
Using the store buffer, stores which are not preceded by other stores can be performed in a single thread cycle.
By \emph{thread cycle}, we denote the time it takes for an instruction to pass through the thread-interleaved pipeline.
Other stores may take two thread cycles to execute.
A bigger store buffer would be able to hide latencies of successive stores at the expense of increased complexity in timing analysis.


\paragraph{Scheduling of Refreshes}
DRAM cells need to be refreshed at least every 64 ms.
A refresh can either be performed by a hardware refresh command, which may refresh several rows of a device at once\footnote{Internally, this still results in several consecutive row accesses.}, or by performing individual row accesses ``manually''.
We opt to do the latter.
This has the advantage  that a single row access takes less time than the execution of a hardware refresh command, thereby improving worst-case latency, particularly for small transfers.
The disadvantage, on the other hand,  is that more manual row accesses have to be performed, incurring a slight hit in bandwidth.

In our device, each bank consists of 8192 rows.
Thus, a row has to be refreshed every $64\textit{ms}/8192=7.8125 {\mu}s$.
At a clock rate of 200 MHz of the memory controller, this corresponds to $7.8125 {\mu}s \cdot (200 \textit{cycles}/{\mu}s) = 1562.5$ cycles.
Since each resource contains two banks, we need to perform two refreshes every $1562.5$ cycles, or one every $781.25$ cycles.
One round of access is $13$ cycles at burst length 4, and includes the access slots to each resource plus a nop command. 
So we schedule a refresh every $\lfloor 781.25/13 \rfloor^{th} = 60^{th}$ round of the backend.
% Instead, we choose to perform refreshes in every $63^{rd}$ slot.
% Why? This allows us to align the refreshes with times in which the thread-interleaved pipeline and the periodic access scheme of the backend are well-aligned. 
% As a consequence, just a small part of the refresh latency actually contributes to the worst-case latency of a memory access.
% We will elaborate on how exactly this works in \refsec{loadstorelatency}.
This way, we are scheduling refreshes slightly more often than necessary.
Scheduling a refresh every $60 \cdot 13$ cycles means that every row, and thus every DRAM cell, is refreshed every $60\cdot 13 \textit{ cycles}\cdot 8192\cdot 2/(200000~\textit{cycles}/\textit{ms}) \leq 63.90\textit{ms}$.
We are thus flexible to push back any of these refreshes individually by up to $0.1\textit{ms} = 20000$ cycles without violating the refreshing requirement.

We make use of this flexibility for loads from the pipeline and when performing DMA transfers:
if a load would coincide with a scheduled refresh, we push back the refresh to the next slot.
Similarly, we skip the first refresh during a DMA transfer and schedule an additional one at the end of the transfer.
This pushes back the refresh of a particular row by at most $60 \cdot 13$ cycles.
%Alternatively, this can be seen as shifting all refreshes, during the DMA transfer, back by $63$ slots or to the end of the transfer.
More sophisticated schemes would be possible, however, we believe their benefit would be slim.
Following this approach, two latencies can be associated with a DMA transfer:
\begin{enumerate}
  \item The time from initiating the DMA transfer until the data has been transferred, and is, e.g., available in the data scratchpad.
  \item The time from initiating the DMA transfer until the thread-interleaved pipeline regains access to the DRAM.
\end{enumerate}
Our conjecture is that latency 1 is usually more important than latency 2.
Furthermore, our approach does not deteriorate latency 2.
For loads sent from the pipeline, the pushed back refreshes become invisible:
as the pipeline is waiting for the data to be returned and takes some time to reach the memory stage of the next instruction, it is not able to use successive access slots of the backend, and thus it is unable to observe the refresh at all.
With this refresh scheme, refreshes do not affect the latencies of load/store instructions, and the refreshes scheduled within DMA transfers are predictable so the latency effects of the refresh can be easily analyzed.  

\subsubsection{Integration with Other Multi-Core Processors}

Several recent projects strive do develop predictable multi-core architectures~\cite{Ungerer10, Wilhelm09, Schoeberl2008265, Hansson09}.
These  could potentially profit from using the proposed DRAM controller.
The frontend described in the previous section makes use of specific characteristics of the PTARM architecture.
However, when integrating the backend with other architectures, we cannot rely on these characteristics.
%In particular, the synchronization of the thread-interleaved pipeline with the backend to hide refreshes is probably infeasible in the projects mentioned above.
%In these projects, one could still use the approach taken for DMA transfers, i.e., pushing refreshes to the end of transfers.
A particular challenge to address is that most multi-core processors use DRAM to share data, while local scratchpads or caches are private.
This can be achieved by sharing the four resources provided by the backend within the frontend.
A particularly simple approach would first combine the four resources into one:
an access to the single resource would simply result in four smaller accesses to the resources of the backend.
This single resource could then be shared among the different cores of a multi-core architecture using predictable arbitration mechanisms such as Round-Robin or CCSP~\cite{Akesson08} or predictable and composable ones like time-division multiple access (TDMA). 
However, sharing the DRAM resources comes at the cost of increased latency.
We investigate this cost in \ref{sec:dma_latency}.
