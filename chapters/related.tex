\label{sec:related_arch_mod}
We are certainly not the first or only one to tackle the unpredictability of computer architecture designs.
In this chapter we survey an abundance of related research to our goal of predictable architectures.
Timing analysis techniques, compiler techniques and architectural techniques all play a role in tackling the unpredictability of computer architectures.
However, we limit the scope of this survey the mainly focus on architectural techniques, as that is the focus of this thesis. 

Adding temporal semantics to programming languages has been the focus of many research proposals, but to the best of our knowledge, we believe this is the first attempt to introduce temporal semantics down at the ISA level. 

%FIXME: Compare to XMOS style handling of interrupts?

\section{Pipeline-Focused Techniques}
\subsection{Static Branch Predictors}
\label{sec:RTBranch}
Dynamic branch predictors cause timing anomalies~\cite{Engblom2003dynbranch} and are difficult to model is because of the \textit{aliasing} of branch points.   
\textit{Aliasing} occurs when two different branches occupy the same branch predictor slot and cause interference.
Burguiere et al.~\cite{Burguiere2005staticbranchpredict} make a case for \textit{static branch prediction} to be used for real-time systems.
This can be done in several ways. 
The simplest form can predict all branches taken or not taken. 
Improvements can include the \textit{Backward Taken, Forward Not Taken} scheme, to improve performance for loops and if statements.
This uses the observation that for loop control branches, almost all backwards branches are taken to return to the loop, and only at the end of the loop are forward branches taken. 
With architecture support for static branch predictions, compilers can analyze code patterns (loops, if-then-else, if then) and insert instruction set constructs to denote the static prediction of each branch.  
The underlying architecture will use that for its prediction, instead of relying on a dynamic hardware unit.
This removes \textit{aliasing} and gives better estimated worst case branch mispredicts. 

Bodin et al.~\cite{Bodin2005staticbranch} use this idea of software static branch prediction to improve WCET of programs.
Intuitively, they aim to remove all branch mispredict penalties from the worst-case path to improve the WCET. 
They propose an algorithm that iterates through the control flow graph to find the worst-case execution path (WCEP). 
Initially, the algorithm finds the worst case path assuming all the branches are mispredicted. 
Then, the algorithm assigns the static branch prediction of all branches on the WCEP to be taken.
The algorithm then iterates again to find the new WCEP until two iterations yield the same WCEP.
Since the algorithm never reassigns assigned branches, it always converges but is not optimal.
The presence of caches can easily effect the WCEP, and each branch prediction reassigned can modify the cache state.
However, the experiments assumed all code and data fit into the caches, thus the effect of caches was not factored into by the algorithm.

\subsection{Superscalar Pipelines}
\label{sec:RTSuperscale}
Superscalar pipelines issue multiple instructions at a time to exploit instruction-level parallelism (ILP). 
In order to keep the pipeline filled, superscalar pipelines typically employ more aggressive techniques to fully utilize the ILP. 
As a result, attempting to model all advanced techniques often leads to either very pessimistic results, or almost infeasible complex models.

Rochange et al.~\cite{Rochange2005superscalar} propose to use instruction pre-scheduling to ease the difficulties of analysis of superscalar pipelines.  
The concept is similar to resetting the pipeline state before each basic block execution. 
This is done by postponing the scheduling of instructions from the next basic block until the instructions from the previous basic block are completed.
If it is possible to remove all timing interference across basic blocks, then the resources needed to model the pipeline can be significantly reduced, as each basic block will start with a consistent initial state.
However, the results assume the absence of caches, which can easily effect execution across basic blocks.  
Furthermore, depending on how many instructions can be in flight at one time, waiting for the pipeline state to be flushed can induce large penalties for programs with a lot of control flow transfer and small basic blocks. 

% \paragraph{A time-predictable execution mode for superscalar pipelines with instruction prescheduling ~\cite{Rochange2005superscalar}}
% \begin{itemize}
% \item What is the background of this work? What is the motivation?\\
% 
% \item What is the main goal?\\
%   They want to reconcile high performance with time
%   predictability. Mainly, they are making out of order superscalar
%   pipelines fit WCET estimation techniques. 
% \item What did they do to achieve this goal?\\
%   They control instruction flow to remove dependence between basic
%   blocks so that any WCET estimation tool would only need to measure
%   or estimate a smaller segment of code.  
% \item How do they evaluate their approach? Does it achieve the goal?
%   Do they compare it with other work?\\
%   They showed a performance comparison of slow down compared to
%   regular out of order superscalar and also against in order scalar
%   pipeline to show performance improvement. 
% \item What other work is listed as future work?\\
% \end{itemize}
% Additional questions:
% \begin{itemize}
%   \item What are the limitations/assumptions of this work?\\
%     They ignore peripheral components (cache memories, TLBs) as well
%     as external events (interrupts) and interactions with the
%     operating system (process scheduling, virtual memory, etc).
%   \item Which parts of a system/design process are modified by this
%     work? (e.g. hardware (which feature?), WCET analysis, scheduling,
%     compiler, programming language, \ldots)\\
% \end{itemize}


% \paragraph{Predictable Out-of-order Execution Using Virtual Traces ~\cite{whitham:08:predOOOwithVirtualTraces}}
% \begin{itemize}
% \item What is the background of this work? What is the motivation?\\
%   The motivation is to improve WCET of complex processors,
%   specifically for out-of-order superscalar pipelines. 
% \item What is the main goal?\\
%   The main goal of this paper is 3 fold. 1) minimizing the pessimism
%   introduced in WCET analysis. 2) increasing CPU throughput that can
%   be guaranteed. 3) minimize CPU modeling cost. 
% \item What did they do to achieve this goal?\\
%   They introduced a VTC (virtual trace controller) to control the
%   progress of the pipeline. They argue that this controller can be
%   used for a CPU of arbitrary complexity. The VTC operates CPU
%   programs as a collection of traces. Traces are paths through the
%   program. Essentially traces are formed by statically predicting
%   branches, in the context of this paper they predict the branches
%   towards the worst case execution path. This way the pipeline
%   optimizations (out of order execution etc) can optimize the worst
%   case path. This achieves goal 2, which is increased the guaranteed
%   throughput. The traces are formed via static branch predictions, and
%   the VTC contains a VTR (virtual trace register) which stores the
%   branch predictions. The pipeline state is reset between traces, so
%   the WCET analysis can be limited to within traces. The side exits
%   are determined by branch mispredicts. Now WCET within each trace and
%   side exit can be measured, and it will be the same execution time,
%   thus no CPU modeling cost is needed. This achieves goals 1 and 3. 
% \item How do they evaluate their approach? Does it achieve the goal?
%   They use the Malardalen WCET benchmark suite, but assume that
%   benchmark programs are single-path programs. They assume using IPET
%   or methods can find the WCET. They use the benchmark programs and
%   run the program on an idealized in order machine to compare the
%   results with their machine. They compare the speed up /slow down and
%   use it to analyze the issues with their approach. 
% \item What other work is listed as future work?\\
% \end{itemize}
% Additional questions:
% \begin{itemize}
%   \item What are the limitations/assumptions of this work?\\
%     They assume WCEP is easily obtained. It also depends on how many
%     traces are formed, and how effective the traces are formed. They
%     assume scratchpads in this work.
%   \item Which parts of a system/design process are modified by this
%     work? (e.g. hardware (which feature?), WCET analysis, scheduling,
%     compiler, programming language, \ldots)\\
%     They need a compiler to compile code into traces and form
%     traces. The hardware is modified with a VTC to control the traces
%     and stall the pipeline between traces. 
% \end{itemize}


Whitham et al. \cite{whitham:08:predOOOwithVirtualTraces} combine the techniques of \emph{instruction pre-scheduling} and \emph{static branch predictions} to propose modifications to an out-of-order superscalar pipeline to provide predictability for single thread execution.  
Instead of basic blocks, the superscalar pipeline pre-schedules instructions across \emph{virtual traces}\cite{Whitham2008formvirtualtraces}. 
\emph{Virtual traces} are program paths with static branch predictions inserted. 
These are usually formed by predicting along the WCEP, similar to the algorithm introduced by Bodin et al~\cite{Bodin2005staticbranch}. 
Each virtual trace can contain a fixed number of branches. 
A VTC (virtual trace controller) is introduced to control the progress of the pipeline.
The VTC contains a VTR (virtual trace register) which stores the branch predictions, and the pipeline state is reset between traces so the WCET analysis can be limited to within traces.
The out-of-order superscalar pipeline is also modified to disallow memory prediction and reordering of branches.
The architecture employs scratchpads instead of caches.  
This allows the execution of traces to run predictably for each different exit (branch mispredict) within a trace.
The architecture shows an improved throughput for most programs when compared to a simple in-order CPU model.
%The authors further present the effects of trace sizes to balance the main path execution time against the costs of side exits. 

% There are issues with this work, as the assumption is the ability to find the WCEP when doing the trace scheduling (how do you obtain numbers for the basic blocks with Out of Order execution, and what if program itself is so complex that the analysis is nearly infeasible?). 
% But the idea of using static branch prediction in combination with WCET could be leveraged. 
% The delayed scheduling of traces to flush the pipeline could be a huge penalty for programs with small tasks that execute frequently. 
% Reducing this delay is thus a trade off between performance of traces vs amount of state to keep to obtain the performance.

\subsection{VLIW architectures}
\label{sec:RTVLIW}
VLIW machines, like superscalars, issue multiple instructions at a time to exploit ILP.
However, unlike superscalars, VLIW machines rely on the compiler to utilize ILP and determine the instructions issued.  
This helps in the predictability of the software because the hardware does minimum reordering or stalling.
 
Yan et al.~\cite{Yan2008VLIW} study the predictability of VLIW machines, and propose changes to the architecture and compiler to improve the predictability. 
They find that although most of the data dependency is scheduled away by the compiler, there are still several factors that limit the predictability on the hardware. 
First, since statically it is not known whether a memory access is a hit or a miss, the hardware still needs to check for it and stall when needed. 
Second, data dependency still exists across compilation units, so the hardware still supports basic data dependency checking to handle those dependencies.
A compilation unit could be a basic block, a loop, a procedure or a region~\cite{Hank:1995:RCI:225160.225189}. 
Finally, if the VLIW uses branch prediction, there is still the need for handling of mis-prediction etc.

As VLIW machines heavily utilize the compiler to improve performance, they propose several compiler techniques to compile programs that lend themselves to better WCET for VLIW architectures. 
First, they use the single-path paradigm proposed by Puschner and Burns~\cite{Puschner:2002:WTP:882515.885528}, and eliminate all non-loop backwards branches with full if-conversions~\cite{Allen:1983:CCD:567067.567085}.  
To mitigate the performance penalty of single-path programming with aggressive hyperblock scheduling~\cite{Mahlke:1992:ECS:144953.144998} to exploit the ILP from VLIW architectures. 
For the data dependencies across compilation units, they use code padding to ensure the execution time is consistent across different paths.    
This will enable easier WCET analysis. 
This work minimally deals with instruction caches, but does not account for the effects of data cache. 

\subsection{Multithreaded Pipelines}
\subsubsection{Thread Scheduling}
With explicit hardware multithreading, the scheduling policy plays a huge role in the predictability of the architecture.
Kreuzinger et al.~\cite{Kreuzinger2000RTmultithread} evaluate the use of different real time scheduling schemes to schedule hardware threads to handle external events. 
They evaluate fixed priority preemptive (FPP), earliest deadline first (EDF), least laxity first (LLF) and guaranteed percentage (GP), which is essentially time sharing the pipeline. 
The architecture used for evaluation is a Java multi-threaded superscalar pipeline with four threads~\cite{Kreuzinger2003multithreadeventhandle}. 
A hardware priority manager is implemented to facilitate the scheduling of threads. 
All real-time threads register their real-time requirements during initialization with the priority manager. 
When the external event occurs, the priority manager schedules the corresponding interrupt service thread, and starts assigning priorities based upon the real time requirements. 
The evaluation criteria to compare scheduling policies is the throughput of the processor. 
The conclusion of the report is that in order to maximize multiple threads on a superscalar machine, the scheduler should try to keep as many threads active as long as possible to leverage thread level parallelism and hide more latencies of pipeline stalls. 
Thus GP does the best because it schedules different active threads each cycle until their percentage runs out, thus it keeps threads alive as long as possible. 
The idea of using hardware threads to service interrupts is novel because of the low overhead to switch contexts. 
By giving the interrupt service routine thread priorities, it may be possible the bound the execution time of higher priority threads. 
Although the dynamic thread scheduling can cause execution time bounds to be imprecise from the effects of timing interference across threads.  
  
El-Haj-Mahmoud et al.~\cite{El-Haj-Mahmoud2005VirtualMultiprocessor} propose a statically scheduled multithreaded architecture called the Real-Time Virtual Multiprocessor(RVMP).   
The idea of a virtual processor is a slice of time on the process. 
The RVMP extends an in-order 4-way superscalar processor to support the partitioning of the pipeline in \emph{space} and \emph{time}.  
In the \emph{space} dimension, the resources of the superscalar can be partitioned to different threads. 
In the \emph{time} dimension, the superscalar resources are time shared, and different threads are scheduled to utilize the resources at different times.  
The hardware extensions to the superscalar pipeline prevent interference between the virtual partitionings.
Scratchpads are employed for predictable memory access latencies, although they assume all accesses go to the scratchpad. 
It is unclear how accesses to shared resources, in particular main memory are dealt with.
A static round-based schedule of the thread execution is constructed to account for the real-time requirements of each thread.
The static schedule utilizes the flexibility of the different time and space partitioning options to allow threads with higher utilization more access to the pipeline. 

\subsubsection{Simultaneous Multithreaded Architectures} 
\label{sec:RTSMT}
Simultaneous Multithreaded Architectures (SMT) attempt to exploit both instruction-level and thread-level parallelism by dynamically scheduling multiple hardware threads onto a multi-way pipeline. 
Each cycle, instructions from different threads can be fetched simultaneously to fully utilize the pipeline.
The dynamic scheduling and aggressive speculation techniques render SMTs almost impossible to use for real-time systems.  
However, several proposals involved slight modifications to architecture to create a \emph{WCET-aware} SMT to be used for real-time systems.  

Barre et al.~\cite{Barre2008RTSMT} propose to assign one explicit hardware thread with the highest priority. 
That thread, called the real-time thread, gains access to any resource whenever it is scheduled. 
Any other thread that is currently occupying the resource will be preempted, and later replayed when the real-time thread is not using it.
The modifications to the SMT include additions to allow the preemption, and also the partitioning of any resource that needs to be shared. 
This gives the highest priority thread the illusion that it has the whole superscalar pipeline to itself, reducing the execution time analysis of the real-time thread to the equivalent of a superscalar architecture. 
Currently the cache effects and branch prediction are listed as future work.  

Hily et al.~\cite{Hily1999} show that out-of-order execution may not be as cost effective as in-order execution on SMT machines.
Thus, Uhrig et al.~\cite{SaschaUhrig2005SMT} propose a similar concept to Barre et al.~\cite{Barre2008RTSMT}, except for an in-order executed superscalar.
Mische et al.\cite{Mische2008SMT} expand this to allow more than one real-time thread running on the SMT architecture.
They do so by essentially time-sharing the highest priority thread slot among the real-time threads.  
The time-sharing schedule is statically constructed to ensure that the real-time threads still provide reasonable WCET guarantees. 
This architecture uses instruction scratchpads without data scratchpads, and no branch predictors, as the branch penalty can be filled with executions from other threads.   
Some issues do arise with the contention of memory access, as it is difficult to partition memory accesses between hardware threads.
Contention between the high priority thread slot and other thread slots are resolved by alerting the memory controller from earlier stages in the pipeline that a high priority thread will issue a memory instruction. 
This way the memory controller can hold off service to the lower priority memory accesses and wait for the high priority access to come.     
However, contention between the real-time threads on the high priority thread slot is not resolved. 
 
% \bigskip
% Although real time SMT proposals have a lot of compelling features, it
% still lacks a formal WCET analysis. All of the proposals have just
% used measured benchmarks to prove performance, and assumed that
% because the high priority thread runs as if no other thread is
% running, so static analysis is possible. In reality, WCET for a
% superscalar processor already contains pessimistic results. Although
% the lack of a branch predictor and in order execution may aid in
% this. Also, in this line of work still only one explicit hardware
% thread slot gets real-time performance, while the other threads'
% performance will have a non-continuous penalty, which is hard to
% categorize. 

\subsubsection{Thread Interleaved Pipelines}
Thread-interleaved pipelines have been proposed and employed in various architectures for research and industry. 
Besides the CDC6600~\cite{CDC6600}, described in section~\ref{section:pret_thread_pipeline}, Lee and Messerschmitt~\cite{lee1987pip}, the Denelcore HEP~\cite{HEP}, the XMOS XS1 architecture~\cite{xmos_xs1}, the Parallex Propeller chip~\cite{parallex} and the Sandbridge Sandblaster~\cite{Erdem02multi-threadedprocessor} all use fine grained thread interleaving for different applications. 
In particular, Lee and Messerschmitt~\cite{lee1987pip} and the Sandbridge Sandblaster~\cite{Erdem02multi-threadedprocessor} propose the use of thread-interleaved pipelines for DSP applications. Lee and Messerschmitt~\cite{lee1987pip} also use a round robin thread scheduling policy while the Sandblaster uses a Token Triggered Threading policy. 
The Token Triggered Threading policy is similar to the round robin scheduling policy in that each hardware thread context can only issue one instruction each in a scheduling cycle. 
However, a token is used to determine which thread's instruction is executed next. 
The XMOS XS1 architecture~\cite{xmos_xs1} allows hardware threads to dynamically be added and removed from the thread scheduling.
They use the dynamically added threads to handle interrupts, which improves the interrupt response latency. 
The XS1 architecture specifies that during execution, there will always be a minimum the number of threads equal to the pipeline depth. 
As explained in section~\ref{section:pret_thread_pipeline}, this removes pipeline hazards to improve throughput.
However, the dynamic thread scheduling can cause each thread's execution frequency to vary depending on the number of threads executing at one time.         

\subsection{Others}
\subsubsection{Virtual Simple Architecture}
Anantaraman et al.~\cite{Anantaraman2003VISA} propose the virtual simple architectures (VISA), which uses dynamic checking to ensure tasks are meeting the deadlines. 
The microarchitecture is split into two modes. 
A simple mode, which conforms to the timing of a hypothetical simple pipeline that is amenable to safe and tight WCET analysis. 
And a high performance mode, in which the architecture can use arbitrary performance-enhancing features.
A task that executes on the VISA is divided into multiple sub-tasks, to gauge progress on the complex pipeline.
Each sub-task is assigned an interim deadline, based on the hypothetical simple pipeline.  
When tasks are executed on the VISA, they are first speculatively executed in high-performance mode.
If no checkpoints are missed, then the high performance mode has met the timing requirements. 
If a checkpoint is missed, the architectures switches to a simple mode to bound the remaining task times to attempt at meeting the timing constraints.
The results show that the high performance mode have average executions times of 3 to 4 times faster than the simple mode.
The authors also discuss possible power savings by scaling the voltage in high performance mode.
However, the tasks and programs must have sufficient slack time to allow for dynamic checking of deadlines, and it is unclear whether the simple mode will always be able to make up the time if the high performance mode misses its checkpoint. 

\subsubsection{Java Optimized Processor}
\label{sec:RTJava}
Schoeberl presents the Java Optimized Processor (JOP)~\cite{jop:wcet} which uses Java for real time embedded systems. 
The design of JOP includes a two level stack cache architecture ~\cite{jop:stack}. 
Instead of using a large register file to store the stack as in PicoJava\cite{McGhan1998PicoJava}, it only uses two registers to store the top two entries of the stack (Register A and Regiser B). 
Leveraging the stack based architecture of JavaVM, whenever an arithmetic operation occurs, the result is always stored back to the top of the stack (Register A). 
Any push or pop operation simply results in a shift of values between the two registers and the stack cache, which only requires one read and one write port for the memory. 
This architecture does not have any data hazards and has very few pipeline stages (no need for an explicit commit/writeback stage). 
Because of the few pipeline stages, it only has a small branch delay penalty, so no branch predictor is used.
All bytecode on JOP is translated into a fixed length microcode. 
Each microcode executes in a fixed number of cycles, independent of its surrounding instruction.
Thus, the WCET analysis only requires a lookup table of bytecode translated into microcode, rendering it a predictable architecture. 

\subsubsection{MCGREP}
Whitham introduces the Microprogrammed Coarse Grained Reconfigurable Processor (MCGREP)~\cite{whitam:06:mcgrep}, which is a reconfigurable predictable architecture.
The architecture of MCGREP contains multiple execution units, but each operation is implemented in microcode. 
The pipeline architecture is extremely simple, reassembling a two stage pipeline with a fetch/decode stage and an execute stage.
No internal state is stored in the pipeline, and instructions do not affect each other's execution time. 
A fast internal RAM without cache is used to store the program and be used as memory for data.    
The microcode operations are predictable in the MCGREP architecture, taking a fixed number of cycles to complete.
Advanced operations can be dynamically loaded as new microcode, which enables application specific instructions to improve performance.
All MCGREP instructions take a fixed number of clock cycles to complete and are unaffected by execution history, making MCGREP a predictable processor. 

\subsubsection{ARPRET}
Andalam et al.~\cite{pretc} introduce the Auckland Reactive PRET (ARPRET) architecture to execute a new language called precision timed C (PRET-C).
PRET-C is a synchronous language extension to C designed to support synchronous concurrency, preemption, and a high-level construct for logical time.
ARPRET extends the Microblaze~\cite{xilinx-microblaze} with a custom Predictable Functional Unit (PFU) that is used for thread scheduling.
The Microblaze is configured to use on-chip memory to achieve predictable memory access latencies. 
The PFU stores the thread contexts of each thread, including the PC, thread status, priority, etc. that are used during each context switch.
By doing the thread scheduling in hardware, ARPRET reduces the thread switching overhead. 
Each thread switch is triggered in software by the C language extensions in PRET-C, and the PFU is used to determine the next context to run.   
Their benchmarks show that the ARPRET achieves predictable execution without sacrificing throughput. 
% \subsubsection{PRET}
% Craven et. al~\cite{Craven_PRET_implementation_2010}  implements PRET thread interleaved pipeline as an open source core using OpenFire 

\section{Memory-Focused Techniques}
\subsection{Caches}
The dynamic behavior of caches cause headaches for real-time systems when trying to predict memory access latencies.
Reineke et al.~\cite{Reineke07TimingPredictability} presented a study on the predictability of different cache replacement policies.
They evaluate the Least Recently Used (LRU), First In First Out (FIFO), Pseudo LRU (PLRU) and Most Recently Used (MRU) replacement policies to determine if LRU was more predictable than other policies, as observed by Heckmann et al.~\cite{Heckmann2003processor}.    
The results confirm that the LRU replacement policy was significantly more predictable than other policies.
Thus, the authors recommend any real-time system with caches to use LRU for its replacement policy. 
This paper also reveals potential for improvement in existing analyses of PLRU and FIFO.

Puat and Decotigny~\cite{Puaut02} propose to use partitioned and lock caches to eliminate the intra- and inter-task interferences when a cache is used. 
Intra-task interferences occur when different memory blocks of the same task compete for cache blocks.
Inter-task interferences occur where a preempting task's memory blocks cause cache reloads in the preempted task.
By using cache partitioning, a part of the cache is reserved for a particular task, and inter-task interference is eliminated.
To eliminate intra-task interference, cache locking is used to lock the contents of cache.
The cache contents can be locked statically, which are fixed at system start for the whole run time, or dynamically, where the contents may change.   
By locking and partitioning caches, the memory access latencies will have more predictable behavior. 

Schoeberl~\cite{jop:jtres_cache} propose to use method caches for the instruction cache of the JOP architecture~\cite{jop:wcet}.
Conventional caches use a cache line as its basic unit of replacement. 
Method caches use \emph{methods} as its unit of replacement. 
A cache can contain different block sizes that are used to store methods. 
There exists a trade-off between performance and predictability for the block sizes of the method cache. 
Methods can occupy more than one block, depending on the method size. 
When a method is called, the cache loads the whole method into the cache, occupying any number of blocks it needs.
The LRU replacement policy is used, since the end of a method usually returns to its parent method. 
When a method is evicted, all blocks it occupies are evicted. 
Thus, the instruction cache is more predictable, because it only changes on method calls. 
Within a method, all instructions are known to be in the cache, so no cache miss results from the instruction cache.
 
%method cache SMT
Metzlaff et al.~\cite{Metzlaff2008MethodcacheSMIT} use a method cache mechanism with the real-time SMT architecture in ~\cite{Mische2008SMT}.
They partition the scratchpad for each different thread so no inter-thread interference will exist.   
Then, they implement the method cache~\cite{Kirner2007ModelFunctionCache} with scratchpads, and give priority to the high priority thread when a filling is needed.
They called this the function scratchpad.  
If the thread is stalled when a method is being filled into the scratchpad, other threads occupy the pipeline, so throughput is preserved with multiple threads. 

\subsection{Scratchpads}
Scratchpads are known to allow more precise WCET analysis~\cite{Wehmeyer2005SPM} because the contents are managed in software. 
Puaut et al.~\cite{Puaut2007SPMvsCache} present a comparison of locked caches and scratchpads, and show that were only subtle differences between the two in terms of performance. 
Most benchmarks give similar WCET estimates. 
The difference stems from the granularity of allocation units. 
For locked caches, the basic allocation unit is a cache line. 
Thus, it is possible to \emph{pollute} the contents of the cache line with contents that are not part of the allocation scheme. 
Also, depending on the associativity of the cache, a cache line that should be locked could possibly be in conflict with another cache line that is also locked, and thus lose its ability to be locked in the cache. 
For scratchpads, the basic allocation unit is only determined by the allocation scheme, so the contents cannot be polluted. 
However, if the basic allocation block is big, it is possible that the allocation block will not fit in the scratchpad at the end due to fragmentation.  
 
% 
% \bigskip
% \textbf{Preemption} - Although scratchpads are more predictable, the
% hardware replacement policy of caches could be advantageous,
% especially for systems that have preemption of tasks. Locked caches
% can lock content in the cache, and when a preemption occurs, the
% hardware can still take advantage of a hardware replacement for lines
% that aren't locked, while still maintaining a certain amount of
% predictability when the original task resumes (because of the locked
% content that aren't replaced). For statically allocated scratchpads,
% the allocation scheme needs to take into account the preempted task,
% and allocate space for that task. This will reduce the available
% allocation space for all other tasks. For dynamically scheduled
% schemes, it will be extremely difficult to find reload points to
% adjust the scratchpad. If a preempted task loads its optimal content
% when it preempts a task, it's unclear what to load back for the
% original task, and when to load it. There needs to be some sort of OS
% that keeps track of what was loaded off the scratchpad. It is unclear
% what overhead this could result in. (\textcolor{red}{Read papers about
%   this} \JR{Write paper about how to do this ;-)})
% 

% Scratchpad (SPM) memories provide time-predictable accesses for data.  
% However, the time-predictable SPM allocation strategies for data only support statically allocated data or stack data.  
% Dynamic data, on the other hand, is only supported by non time-predictable allocation schemes if whole-program pointer analysis identifies every memory operation that could access each variable.
 
Whitham and Audsley~\cite{whitham:09:timepredLoadStore} introduce a hardware scratchpad memory management unit (SPMMU) that manages the transfers of data between memory and the data scratchpad to eliminate \emph{pointer aliasing} and \emph{pointer invalidation}.
\emph{Pointer aliasing} occurs when the same memory location is referenced using different names (pointers). 
\emph{Pointer invalidation} occurs when an object in a memory location is moved out from that memory location.  
As a result, an alias that points to the object before the move, ends up pointing to an incorrect object.
They propose to separate \emph{logical addresses} (used by the program) and \emph{physical addresses} (identifying where an object resides).  
The SPMMU maintains a table mapping the logical address and physical address.   
Although the SPMMU resides in hardware, its contents are controlled by software via explicit OPEN and CLOSE commands in the code.  
The user specifies the base address for the object, the size of the object and the physical address at which the object is being loaded to. 
The SMMU then performs the transfer, and updates an internal table mapping the logical address to the new physical location of the object.  
This simplifies analysis because it eliminates the need for whole-pointer analysis in the program. 

% \paragraph{Implementing Time-Predictable Load and Store Operations~\cite{whitham:09:timepredLoadStore} }
% 
% \begin{itemize}
% \item Basic definitions: \\
% \textbf{Pointer aliasing:} The same memory location may be referenced using different names (pointers). 
% \textbf{Pointer invalidation:} An object in a memory location is moved out from that memory location.  As a result, an alias that points to the object before the move, ends up pointing to an incorrect object. 
% \textbf{Whole-program Pointer analysis:} Determine which pointers can point to which variables and storage locations in the entire program.
% \item What is the background of this work? What is the motivation?\\
% Scratchpad (SPM) memories provide time-predictable accesses for data.  However, the time-predictable SPM allocation strategies for data only support statically allocated data or stack data.  Dynamic data, on the other hand, is only supported by non time-predictable allocation schemes if whole-program pointer analysis identifies every memory operation that could access each variable. 
% \item What is the main goal? \\
% The objective of this paper is to implement a scratchpad memory management unit that transfers data between external memory and scratchpad memories such that pointer aliasing and pointer invalidation are eliminated. 
% This approach lifts some of the restrictions (i.e. eliminate pointers entirely from program code or no support for dynamic data) forced by the limitations of WCET analysis. 
% \item What did they do to achieve this goal?\\
% They implemented a scratchpad memory management unit (SPMMU) in hardware that performs a mapping from logical addresses (used by the program) to physical addresses (identifying where an object resides).  In addition, the SPMMU performed DMA transfers between the external memory and SPM.  The program dictates when the transfers take place via explicit OPEN and CLOSE commands in the code. The user specifies the base address for the object, the size of the object and the physical address at which the object is being loaded to.  The SMMU then performs the transfer but updates an internal table mapping the logical address to the new physical location of the object.  
% \item How do they evaluate their approach? Does it achieve the goal? Do they compare it with other work?\\
% They use three approaches to evaluate their work.  
% Their first approach simply looks at the hardware cost incurred.  In particular, they observe that since the mapping must take place in the same cycle that the request is made, it must be implemented as combinational logic; hence, a part of the critical path.  They determine this critical path and its effect on the clock frequency. 
% The second approach compares their approach for a particular function of a JPEG decode algorithm with a data cache with estimated access latencies.  They show that the SMMU is not as effective as a data cache in ideal conditions, they are much better in the worst case. 
% The third approach is a case-study implementation of the entire JPEG decode algorithm with the SMMU. 
% \item What other work is listed as future work?
% Integrating one of the SPM allocation techniques with the SMMU to determine the where to place OPEN and CLOSE commands. 
% \end{itemize}
% Additional questions:
% \begin{itemize}
%   \item What are the limitations/assumptions of this work?\\
% The user must specify the size of the object and the physical address. 
% They claim that there are algorithms that determine the best physical address, but I haven't read that work yet. 
%   \item Which parts of a system/design process are modified by this work? (e.g. hardware (which feature?), WCET analysis, scheduling, compiler, programming language, \ldots)\\
% They require the following modifications: 1) hardware, 2) WCET analysis, and 3) compilers to generate the OPEN/CLOSE commands. 
% \end{itemize}

% \subsubsection{Static allocation schemes}
% \label{sec:SPM_static_allocation}
% Static allocation schemes allocate the content on the scratchpad a
% priori, and the content stays the same throughout the whole
% execution. Most static scratchpad allocation schemes
% (\textcolor{red}{Find some citations}) use some sort of heuristic to
% find the most commonly executed instructions or data structures, and
% then allocate them statically on the scratchpad to improve the ACET
% (average case execution time). Suhendra et
% al.~\cite{Suhendra2005WCETSPM} (Abhik Roychoudhury) was the first to
% propose using static data scratchpad allocation to improve the worst
% case execution time. They first identified the difficulty in
% constructing such an algorithm that optimally allocates data to
% improve the WCET because once data is allocated on the SPM, the worst
% case execution path will change. Also, it's possible that the WCEP
% that's resulted from static analysis is actually infeasible, so the
% detection of infeasible task is also needed. As a result, they
% proposed a greedy method that allocates contents of the SPM in attempt
% to reduce the WCET. They first assume that no allocation is done for
% any memory location. They find the WCEP, and allocate the most used
% data block (not too sure of what size, does it depend on the data
% structure? Is it fixed size?)  from that path onto the SPM, then
% reiterates the algorithm to find the new WCEP. This iterates until the
% SPM space is exhausted. Note that is is sub-optimal because allocated
% content will not be reconsidered. This means that if the first WCEP
% and the second WCEP don't share that data structure, the first
% allocation does not contribute to improving the WCEP.
% 
% Patel et al.~\cite{Patel2008PRETSPM} proposed another static
% allocation scheme based on \cite{Suhendra2005WCETSPM}, except that the
% allocation criteria wasn't to minimize the WCET, but to meet all
% deadlines in the program. The greedy approach worked by synthesizing
% timing constructs (deadline blocks) into test programs. The algorithm
% works by identifying the deadline blocks that miss their
% deadline. Then a profile is constructed based on the number of
% accesses to a data block in the missed deadline blocks. Based on the
% profile, a data block is selected to be allocate on the scratchpad,
% and then the algorithm reiterates itself, until either all deadlines
% can be guaranteed to be met, or if the SPM exhausts its space. In the
% first case, the remaining space can be optimized by another method. In
% the second case, the program is deemed un-schedulable, and the
% deadlines cannot be met. 

\subsection{DRAM}
DRAM cells leak charge and have to be refreshed periodically to retain their state.
However, the refreshes of DRAMs stall other DRAM accesses, and potentially close DRAM rows, which require additional precharges to reopen them.
This causes DRAMs to be unpredictable for real-time systems, as the DRAM refreshes are usually controlled in hardware. 
Bhat and Muller~\cite{Bhat2010PredictableDRAM} tackled this specific issue of DRAM refreshes by scheduling burst refreshes.
They account for the DRAM refresh requirements in the software, and schedule refresh tasks to handle DRAM refreshes predictably. 
Two implementations are provided. 
The first is a pure software implementation, and use RAS-only refreshes to manually refresh the DRAM rows during the refresh task. 
The second implementation uses a hybrid software-hardware solution, where the software initiates a hardware DRAM refresh. 
Depending on the application needs, each refresh can contain smaller bursts at the cost of scheduling more refreshes. 
By scheduling the DRAM refresh, other DRAM accesses are more predictable because no conflict will arise from refreshes.  

Akesson et al.~\cite{Akesson2007CODES,Akesson2009DSD,Akesson2010} introduce the Predator, a predictable SDRAM memory controller. 
Here,``predictable'' provides a guaranteed maximum latency and minimum bandwidth to each client, independent of the behavior of each client. 
Standard DDR2 SDRAM memory controllers schedule the requests of the different components dynamically. 
Predicting the execution time of a particular component in such a system is difficult, because of interference on the shared DRAM resource. 
Predator is a hybrid between static and dynamic memory controllers.
Predator precomputes a set of of read and write groups with corresponding static sequences of SDRAM commands.
These static sequences allow the computing of latency bounds, and are scheduled by the backend dynamically. 
As predictor is meant to service multiple clients, requests by different clients are scheduled using a Credit-Controlled Static-Priority arbiter (CCSP). 
This provides a maximum latency and bandwidth to the clients based upon the guarantees of the backend. 
The front-end also may delay each response by the back-end up to its worst-case bound.
This eliminates interactions between different requestors.

Paolieri et al.~\cite{Paolieri2009ESL} present the Analyzable Memory Controller (AMC), which uses a very similar approach to the Predator. 
The main difference between AMC and Predator is that the AM uses a Round-Robin (RR) arbiter, instead of a CCSP arbiter employed in Predator. 
The RR arbiter provides the same latency and bandwidth guarantees to all clients while the CCSP provides better latency guarantee for high priority tasks. 
