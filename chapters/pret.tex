In this chapter we present the design principles of a PREcision Timed (PRET) Machine.
Specifically, we discuss the implementation of a predictable pipeline and memory controller, and present timing extensions to the ISA. 
It is important to understand why and how current architectures fall short of timing predictability and repeatability.
Thus, we first discuss the common architectural designs and their effects on execution time, and point out some key issues and trade offs when designing architectures for predictable and repeatable timing.

\section{Pipelines}
The introduction of pipelining vastly improves the performance of processors.
Pipelining increases the number of instructions that can be processed at one time by splitting up instruction execution into multiple steps.
It allows for faster clock speeds, and improves instruction throughput compared to single cycle architectures.
Ideally each in processor cycle, one instruction completes and leaves the pipeline as another enters and begins execution. 
In reality, different pipeline hazards occur that reduce the throughput and create stalls in the pipeline.
The techniques introduced to mitigate the penalties of pipeline hazards greatly effect to the timing predictability and repeatability of architectures.     
We analyze several commonly used techniques to reduce the performance penalty from hazards, and show their effects on execution time and predictability. 

\subsection{Pipeline Hazards}
\label{sec:pipeline_hazards}
\subsubsection{Data Hazards}

\begin{wrapfigure}{r}{0.5\textwidth}
  \vspace{-30pt}
  \begin{center}
    \includegraphics[scale=.65]{figs/sample_data_dependent_code}
  \end{center}
  \vspace{-3mm}
  \caption{Sample code with data dependencies}
  \label{fig:sample_data_dependent_code}
\end{wrapfigure}

Data hazards occur when the data needed by an instruction are not yet available.
Pipelines begin the execution of instructions before preceding ones are finished, so consecutive instructions that are data-dependent could simultaneously be executing in the pipeline.
For example, the code in figure~\ref{fig:sample_data_dependent_code} shows assembly instructions from the ARM instruction set architecture (ISA) that each depend on the result of its previous instruction.
Figure~\ref{fig:data_depend_execution_non_interleaved} shows two ways data hazards are commonly handled in pipelines. 

\begin{figure}
\vspace{-20pt} 
\begin{center}
\includegraphics[scale=.6]{figs/data_depend_execution_non_interleaved}
\end{center}
\vspace{-3mm}
\caption{Handling of data dependencies in single threaded pipelines}
\label{fig:data_depend_execution_non_interleaved}
\end{figure}

In the figure, time progresses horizontally towards the right.
Each column represents a processor cycle.
Each row represents an instruction that is fetched and executed within the pipeline.
Each block represents the instruction entering the different stages of the pipeline -- fetch (F), decode (D), execute (E), memory (M) and writeback (W).   
We assume a classic five stage RISC pipeline.

A simple but effective technique stalls the pipeline until the previous instruction completes.
This is shown in the top of figure~\ref{fig:data_depend_execution_non_interleaved}, as delays are inserted to wait for the results from previous instructions.
The dependencies between instructions are explicitly shown in the figure to make clear why the pipeline delays are necessary.
The performance penalty incurred in this case comes from the pipeline delays inserted.

\emph{Data forwarding} is commonly used to mitigate the delays when data hazards occur.
Pipelines split up the execution of instructions into different execution stages. 
Thus, the results from an instruction could be ready, but waiting to be committed in the last stage of the pipeline.
Data forwarding introduces backwards data paths in the pipeline, so earlier pipeline stages can access the data from instructions in later stages that have not yet committed.
This greatly reduces the delays inserted in the pipeline.
The circuitry for data forwarding usually consists of the backwards data paths and multiplexers in the earlier pipeline stages to select the correct data to be used.    
The pipeline controller dynamically detects whether a data dependency exists, and changes the selection bits of the multiplexers accordingly.

The bottom of figure~\ref{fig:data_depend_execution_non_interleaved} shows the execution sequence of the previous example in a pipeline with data forwarding.
No pipeline delays are inserted for the first \emph{sub} and \emph{ldr} instruction because the data they depend on are forwarded.
However, delays are still inserted for the second \emph{sub} instruction after the \emph{ld} instruction.
For longer latency operations, such as memory accesses, the results are not yet available to be forwarded by the forwarding paths, so pipeline delays are still required. 
This illustrates the limitations of data forwarding.
They can address data hazards that result from pipelining, such as read-after-write register operations, but they cannot address data hazards that result from long latency operations, such as memory operations.
More involved techniques such as the out-of-order execution or superscalars are required to mitigate the effects of long latency operations.

The handling of data hazards in pipelines can cause instructions to exhibit dynamic execution times.  
For example, figure~\ref{fig:data_depend_execution_non_interleaved} shows the \emph{sub} instruction, in both top and bottom figures, exhibiting different execution times. 
To determine the execution time of instructions on pipelines that stall for data hazards, we need to determine when a stall is inserted, and how long the pipeline is stalled for.
Stalls are required when the current instruction uses the results of a previous instruction that is still in execution in the pipeline.
Thus, depending on the pipeline depth, a window of previous instructions needs to be checked to determine whether any stalls are inserted.     
The length of the stall is determined by the execution time of the dependent instructions, because the pipeline will stall until those instructions complete.
Data forwarding does not remove the data hazards, but only reduces the number of stalls required to take care of the data hazards.  
Thus, to determine the execution time when data forwarding is used, timing analysis needs to determine when the data forwarding circuitry cannot not forward the data for data hazards.
%This can similarly be done by observing the window of instructions not yet committed in the pipeline.  
%The difference is, instead of all data dependencies, only data dependencies from long latency operations need to be detected.

Both stalling and forwarding cause the execution time of instructions to depend on a window of previous instructions.
The deeper the pipeline, the larger the window.
Thus, execution time analysis needs to model and account for this additional window of instructions on pipelined architectures that use stalling or forwarding to handle the data hazards. 

\subsubsection{Control Hazards}
\begin{wrapfigure}{r}{0.5\textwidth}
  \vspace{-20pt}
  \begin{center}
    \includegraphics[scale=.65]{figs/sample_gcd_code}
  \end{center}
  \vspace{-3mm}
  \caption{GCD with conditional branches}
  \label{fig:sample_gcd_code}
\end{wrapfigure}

Branches cause control-flow hazards (or control hazards) in the pipeline; the instruction after the branch, which should be fetched the next cycle, is unknown until after the branch instruction is completed.
Conditional branches further complicates matters, as whether or not the branch is taken depends on an additional condition that could also be unknown when the conditional branch is in execution. 
The code segment in figure~\ref{fig:sample_gcd_code} implements the \emph{Greatest Common Divisor} (GCD) algorithm using the conditional branch instructions \emph{beq} (branch equal) and \emph{blt} (branch less than) in the ARM ISA.  
Conditional branch instructions in ARM branch based on conditional bits that are stored in a processor state register.
Those conditional bits can be set based on the results of standard arithmetic instructions~\cite{armrefman}.
The \emph{cmp} instruction is one such instruction that subtracts two registers and sets the conditional bits according to the results.
The GCD implementation shown in the code uses this mechanism to determine whether to continue or end the algorithm.
Figure~\ref{fig:branch_execution_non_interleaved_pipeline} shows the execution of the conditional branches from our example, and demonstrates two commonly used techniques to handling control hazards in pipelines. 
To show only the timing effects of handling control hazards, we assume an architecture with data forwarding that handles data hazards.
As there are no long latency instructions in our example, all stalls observed in the figure are caused by the handling of control hazards.  

\begin{figure}
\begin{center}
\noindent\makebox[\textwidth]{%
\includegraphics[scale=.58]{figs/branch_execution_non_interleaved_pipeline}}
\end{center}
\vspace{-3mm}
\caption{Handling of conditional branches in single threaded pipelines}
\label{fig:branch_execution_non_interleaved_pipeline}
\end{figure}

Similar to data hazards, control hazards can also be handled by stalling the pipeline until the branch instruction completes. 
This is shown on the left of figure~\ref{fig:branch_execution_non_interleaved_pipeline}. 
%The dependencies between instructions are explicitly shown to make clear why the pipeline delays are necessary.
Branch instructions typically calculate the target address in the execute stage, so two pipeline delays are inserted before the fetching of the \emph{blt} instruction, to wait for \emph{beq} to complete the target address calculation. 
The reasoning applies to the two pipeline delays inserted before the \emph{sub} instruction. 
The performance penalty (often referred to as the \emph{branch penalty}) incurred in this case is the two delays inserted after every branch instruction, to wait for the branch address calculation to complete.

To mitigate the branch penalty, some architectures require the compiler to insert one or more non-dependent instructions after each branch instruction.
These instruction slots are called branch delay slots, and are always executed before the pipeline branches to the target address. 
This way, instead of wasting cycles to wait for the target address calculation, the pipeline continues to execute useful instructions before it branches.
However, if the compiler cannot place useful instructions in the branch delay slot, \emph{nops} need to be inserted into those slots to ensure correct program execution.
Thus, branch delay slots are less effective for deeper pipelines, because more delay slots need to be filled by the compiler to account for the branch penalty.
  
Instead of stalling, \emph{branch predictors} are commonly employed to predict the branch condition and target address so the pipeline can speculatively continue execution. 
Branch predictors internally maintain some form of state machine that is used to determine the prediction of each branch.  
The internal state is updated after each branch according to the results of the branch. 
Different prediction schemes have been proposed, and some can even accurately predict branches up to 98.1\%~\cite{Mcfarling_Branch_predict}.  
If the branch prediction is correct, no penalty is incurred for the branch because the correct instructions are speculatively executed.  
%With branch predictor, the pipeline fetches the next instruction based upon the results of the branch prediction, and continues to execute speculatively.
However, when the prediction is incorrect (often referred as a \emph{branch midpredict}), the speculatively executed instructions are flushed, and the correct instructions are re-fetched into the pipeline for execution.

The right of figure~\ref{fig:branch_execution_non_interleaved_pipeline} shows the execution of GCD in the case of a branch misprediction.
The \emph{beq} branch is predicted to be taken, so the \emph{add} and \emph{mov} instructions from label \emph{end} are directly fetched into execution. 
When \emph{beq} progresses past the execute stage, \emph{cmp} has forwarded its results used to determine the branch condition, and the branch target address has been calculated, so the branch is resolved.
At this point, the misprediction is detected, so the \emph{add} and \emph{mov} instruction are flushed out of the pipeline. 
The next instruction from the correct path, the \emph{blt} instruction, is immediately re-fetched, and execution continues.
The performance penalty of branch mispredictions is derived from the number of pipeline stages between instruction fetch and branch resolution.  
In our example, the misprediction penalty is 2, as branches are resolved after the execute stage.
This penalty only occurs on a branch mispredict, thus branch predictors with high success rates typically improve average performance of pipelines drastically, compared to architectures that simply stall for branches.
%However, for more complex architectures with caches or other hardware states, the effects of incorrectly fetched instructions on the state of the processor less well-known and studied. 

Stalling and branch predicting exhibit vastly different effects on execution time.    
When stalls are used to handle control hazards, the execution time effects are static and predictable.   
The pipeline will simply \emph{always} insert pipeline delays after a branch instruction.
Thus, no extra complexity is added to the execution time analysis; the latency of branch instructions simply needs to be adjusted to include the branch penalty.
On the other hand, if a branch predictor is employed, the execution time of each branch will vary depending on the result of the branch prediction.  
To determine the success of a branch prediction, both the prediction and the branch outcome, both of which can dynamically change in run-time, must be known.   
Program path analysis can attempt to analyze the actual outcome of branches statically from the program code. 
The predictions made from the branch predictor depend on the internal state stored in the hardware unit.
This internal state, updated by each branch instruction, must be explicitly modeled in order to estimate the prediction. 
If the predictor state is unknown, the miss penalty must conservatively be accounted for.
There has been work on explicitly modeling branch predictors for execution time analysis~\cite{Mitra_branch_wcet_2002}, but the results only take into account the stalls from the branch penalty. 
Caches and other processor states are assumed to be perfect. 
In reality, the speculative execution on the predicted program paths lead to further complications that need to be accounted for.
Other internal states exist in the architecture that could be affected by speculatively executing instructions.
For example, if caches are used, their internal state could be updated during speculative execution of a mispredicted path.
As architectures grow in complexity, the combined modeling of all hardware states in the architecture often leads to an intractable explosion in state space for the analysis.    
This makes a tight static execution time analysis extremely difficult, if not impossible.

The difference in execution time effects between stalling and employing a branch predictor highlights an important trade off for architecture designs.    
It is possible to improve average-case performance by making predictions, and speculatively executing based upon them.
However, this comes at the cost of predictability, and a potential decreasing of the worst-case performance.  
For real-time and safety critical systems, the challenge remains to improve worst-case performance while maintaining predictability, and how pipeline hazards are handled plays a key role in tackling this challenge. 
 
Although less often mentioned, the presence of interrupts and exceptions in the pipeline also creates control hazards. 
Exceptions can occur during the execution of any instruction and change the control flow of the program to execute the exception handler.
For single threaded pipelines, this means that all instructions fetched and not committed in the pipeline are speculative, because when an exception occurs, all uncommitted instructions in the pipeline become invalid.
Pipelines often handle this by flushing all instructions and fetching the exception handler for execution.      
These effects are acknowledged, but often ignored in static analysis because it is simply impossible to model every possible exception and its effect on the architecture states. 
 
\subsubsection{Structural Hazards}
\emph{Structural hazards} occur when a processor's hardware component is needed by two or more instructions at the same time. 
For example, a single memory unit accessed both in the fetch and memory stage results in a structural hazard. 
The design of the pipeline plays an integral part in eliminating structural hazards. 
For example, the classic RISC five stage pipeline only issues one instruction at a time, and uses separate instruction and data caches to avoid structural hazards.
Structural hazards are generally much more prevalent in architectures that issue multiple instructions at a time.
If structural hazards cannot be avoided, then the pipeline must stall to access the contended hardware component sequentially.
The execution time effects of structural hazards are specific to how contention is managed for each pipeline design.
Here we omit a general discussion of the timing effects, and later address them specifically for our proposed architecture. 

\subsection{Pipeline Multithreading}
Discussed above, \emph{data forwarding} and \emph{branch prediction} are simple techniques employed to handle pipeline hazards. 
Advanced architectures, such as \emph{superscalar} and \emph{VLIW} machines, employ more complex mechanisms to improve the average performance of the architecture.  
Both architectures issue multiple instructions every cycle, and superscalar machines dynamically execute instructions out-of-order if no dependency is detected.    
These architectures exploit \emph{instruction-level parallelism} to overlap the execution of instructions from a single thread whenever possible.         
On the contrary, \emph{multithreaded architectures} exploit \emph{thread-level parallelism} to overlap the execution of instructions from different hardware threads. 
Each hardware thread in a multithreaded architecture has its own physical copy of a processor state, such as the register file and program counter.
When a pipeline hazard arises from the execution of a hardware thread, another hardware thread can be fetched for execution to avoid stalling the pipeline. 
This improves the instruction throughput of the architecture.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=.8]{figs/multithreaded_pipeline_block}
\end{center}
\vspace{-10pt}
\caption{Simple Multithreaded Pipeline}
\label{fig:multi-thread pipeline simplified}
\end{figure}
Figure~\ref{fig:multi-thread pipeline simplified} shows the implementation of a simple multithreaded pipeline.
It contains 5 hardware threads, so it has 5 copies of the Program Counter (PC) and register files.
The rest of the pipeline remains similar to a classic five stage RISC pipeline, with the addition of a few multiplexers used to select the thread states.
Thus, the extra copies of the processor state and multiplexers are most of the hardware additions needed to implement hardware multithreading.
When a hardware thread executes in the pipeline, its corresponding thread state is passed into the pipeline to be used.
In most of this thesis, the term \emph{threads} to refer to the explicit hardware threads that have hardware copies of the thread state.
This is not to be confused with the common notion of \emph{threads}, which describes software contexts managed by an operating system, with its states stored in memory.
It will be explicitly noted when we refer to the software notion of threads. 
%The selection of threads for execution is one of the most important factors to fully utilize thread-level parallelism.
%If a thread is stalled waiting for memory access but gets selected to execute in the pipeline, then that instruction slot is wasted and the processor isn't fully utilized.
Ungerer et al.~\cite{Ungerer:2003:survey_multithreading} survey different multithreaded architectures and categorize them based upon the \emph{thread scheduling} policy and the \emph{execution width} of the pipeline.

The \emph{thread scheduling} policy determines which threads are executing, and how often a context switch occurs.  
\emph{Coarse-grain} policies manage threads similarly to the way operation systems manage software threads.
A thread gains access to the pipeline and continues to execute until a context switch is triggered.
Context switches occur less frequently via this policy, so fewer threads are required to fully utilize the processor.
Different coarse-grain policies trigger context switches with different events. 
Some policies trigger context switches on dynamic events, such as a cache miss or an interrupt; some policies trigger context switches on more static events, such as specialized instructions.
\emph{Fine-grain} policies switch context much more frequently -- some as frequently as every processor cycle.
The \emph{execution width} of the pipeline is the number of instructions fetched each cycle.  
Multithreaded architectures with wider pipeline widths can fetch all instructions a single thread, or mix instructions from different threads.
The Sumultanous Multithreaded (SMT) architecture~\cite{Tullsen1995SMT} is an example where instructions are fetched from different threads each cycle.

Multithreaded architectures present several challenges for static execution time analysis.
As figure~\ref{fig:multi-thread pipeline simplified} illustrates, threads share the hardware components within the pipeline.
If a hardware component, such as a branch predictor, maintains internal state, that internal state can be modified by all threads in the pipeline.
As the internal states of the hardware components affect the execution time of the individual instructions, each thread can affect the execution time of all threads in the pipeline. 
If the threads' execution times are interdependent, their timing cannot be separately analyzed.
As a result, in order to precisely model the hardware states, the execution order of instructions from all threads needs to be known.
The interleaving of threads depends heavily on the thread scheduling policy, execution width, and hazard handling logic employed in the pipeline.
The compounding effect of these can create an overwhelming combination of possible thread interleavings, making static timing analysis nearly impossible, even if only a conservative estimation is desired.   

Nonetheless, we contend that thread-level parallelism (TLP) \emph{can} be exploited to handle pipeline hazards predictably. 
Even the most sophisticated architectures that fully exploit instruction-level parallelism (ILP) cannot guarantee enough parallelism in a single instruction stream to remove all stalls caused by pipeline hazards. 
This is known as the \emph{ILP Wall}~\cite{Wall:1991:LIP:106975.106991}. 
Conventional multithreaded architectures use coarse-grain thread scheduling policies to dynamically exploit TLP when there is not enough ILP to be exploited.    
However, the compounding effects of the combined architectural features lead to unpredictable architectural timing behaviors.
Instead, a \emph{thread-interleaved pipeline} fully exploits TLP with a fine-grained thread scheduling policy.
We show that with several predictable architectural adjustments to the thread-interleaved pipeline, we can achieve a fully time-predictable pipeline 
with deterministic execution time behaviors.     

\subsection{A Predictable Thread-Interleaved Pipeline}
\label{section:pret_thread_pipeline}
Thread-interleaved pipelines use a fine-grain thread scheduling policy; every cycle a different hardware thread is fetched for execution.
A round robin scheduling policy is often employed to reduce the context switch overhead every cycle.     
The thread-interleaved pipeline is known for implementing the peripheral processors of the Cray Multi-Threaded Architecture (MTA) multiprocessors~\cite{CDC6600}.
Each the ``peripheral processor'' is implemented as a hardware thread.     
Interacting with input/output peripherals often lead to idle processor cycles that waits for the peripherals' responses.
By interleaving several threads, thread-level parallelism is fully exploited, and the idle cycles can be used for simultaneous interaction with multiple input/output devices.       
Figure~\ref{fig:execution_thread_interleaved_pipeline} shows an example execution sequence from a 5 stage single width thread-interleaved pipeline with 5 threads.
\begin{figure}[h]
    \begin{center}
    \includegraphics[scale=.55]{figs/thread-interleaved-execution}
  \end{center}
  \vspace{-10pt}
  \caption{Sample execution sequence of a thread-interleaved pipeline with 5 threads and 5 pipeline stages}
  \label{fig:execution_thread_interleaved_pipeline}
\end{figure}

The same code segments from figure~\ref{fig:sample_gcd_code} and figure~\ref{fig:sample_data_dependent_code} are used in this example. 
Threads 0, 2 and 4 execute GCD (figure~\ref{fig:sample_gcd_code}) and threads 1 and 3 execute the data dependent code segment (figure~\ref{fig:sample_data_dependent_code}).
The thick arrows on the left show the initial execution progresses of each thread at cycle 0.
We observe from the figure that each cycle, an instruction from a different hardware thread is fetched in round robin order.
By cycle 4, each pipeline stage is occupied by a different hardware thread.
The fine-grained thread interleaving and round robin scheduling combine to form this important property for thread-interleaved pipelines, which provides the basis for a timing predictable pipeline design.

The interleaving of threads by itself does not guarantee timing predictability for the pipeline.  
Shared hardware components or a selective thread execution policy can easily allow the execution time of threads to be affected by each other.    
As previously discussed, a combined timing analysis of all threads in the pipeline is extremely difficult, if not impossible.
In order for multithreaded architectures to achieve predictable performance, threads must be temporally isolated from one another. 
Temporal isolation removes cross-thread timing dependencies to allows timing analysis of threads independently.
This enables a simple and more precise execution time analysis.  
We refine several features on the thread-interleaved pipeline to temporally isolate the threads and predictably handle pipeline hazards.
This establishes a time-predictable thread-interleaved pipeline.

\subsubsection{Control Hazards}
By interleaving enough threads, control hazards can be completely removed in thread-interleaved pipelines.
This can be observed from the execution sequence shown in figure~\ref{fig:execution_thread_interleaved_pipeline}.

At cycle 2, a \emph{blt} instruction from thread 2 is fetched into the pipeline.
In a single-threaded pipeline, a stall or branch prediction would be required before the next instruction fetch.
However, as the figure illustrates, the next instruction fetched (\emph{ldr}) at cycle 3 belongs to a different thread.
There is no control hazard in this case, because the \emph{ldr} instruction does not rely on the branch results of the \emph{blt} instruction.  
Thus, no stall or branch prediction is needed to fetch this instruction.
In fact, the branch result from \emph{blt} is not needed until cycle 7, when thread 2 is fetched again.
By this point, the branch has already been resolved, so no control hazard is caused from the \emph{blt} instruction.
The next fetched instruction from thread 2 is \emph{always} from the correct program path.
In this way, the control hazards from branches are eliminated.  
  
The interleaving of threads also eliminate control hazards in the presence of exceptions.
If the pipeline detects an exception for the \emph{blt} instruction in its writeback stage (cycle 6), the control flow for thread 2 will be changed to handle the exception.  
Because no other instruction in the pipeline belongs to thread 2 at cycle 6, no instruction needs to be flushed.  
This reveals an important property our timing predictable pipeline, that \emph{no instruction is speculatively executed}.
The next instruction fetch from thread 2 does not occur until cycle 7.
At that point, any control flow change, including one caused by an exception, is already known.
Therefore, the correct program path is always executed.

The minimum number of threads required to eliminate control hazards depends on the number of the pipeline stages.
Conservatively, interleaving the same number of threads as pipeline stages will always remove control hazards.
Intuitively, this is because at any point in time, each stage of the pipeline with be executing an instruction from a different hardware thread. 
Thus, no explicit dependency will exist between instructions in the pipeline. 
Lee and Messerschmitt~\cite{lee1987pip} further showed that it is possible to use one less thread than the number of pipeline stages for certain implementations. 
From here on, when we refer to the thread-interleaved pipeline, we assume enough threads to remove \emph{explicit} dependencies between instructions in the pipeline.
	
Because control hazards are eliminated, branch predictors are not needed in our pipeline design. 
Removing the branch predictor contributes to the temporal isolation of threads, as the shared internal state of the branch predictor can create \emph{implicit} dependencies between threads.  

\subsubsection{Data Hazards}
In a thread-interleaved pipeline, data hazards that stem from the pipeline of instructions are removed.  
The same reasoning for control hazard elimination is applied here, that no \emph{explicit} dependencies exist between instructions in the pipeline, 
However, long latency operations can still cause data hazards in a thread-interleaved pipeline. 
This happens when a long latency operation is not completed before the next instruction fetch from the same thread.
Although thread-interleaved pipelines can continue to fill the pipeline with other threads, if all threads simultaneously execute a long latency operation, then no thread will be available to fill the pipeline. 

To maximize pipeline utilization and instruction throughput, thread-interleaved pipelines can mark threads inactive for long latency operations.   
However, this dynamic thread scheduling leads to non-trivial timing effects for the pipeline 
First, the number of active threads can fall below the minimum number of threads required to remove explicit dependencies of instructions in the pipeline.
In this case, the eliminated control and data hazards are now reintroduced, and hazard handling logic, like the branch predictor, is required again. 
%As a result, the timing effects of branch prediction etc are re-introduced in the pipeline. 
%By allowing threads to be inactive, it is possible for the number of active threads to fall below this minimum. 
This can be circumvented by inserting pipeline stalls when the number active threads falls below the minimum.
This is illustrated in figure~\ref{fig:three_thread_pipeline}.
\begin{figure}[h]
  \vspace{-10pt}
  \begin{center}
    \includegraphics[scale=.6]{figs/three_thread_pipeline}
  \end{center}
  \vspace{-10pt}
  \caption{Execution of 5 threads thread-interleaved pipeline when 2 threads are inactive}
  \label{fig:three_thread_pipeline}
\end{figure}
In the figure, 3 (out of 5) threads are interleaved through a 5 stage pipeline. 
We assume that the other 2 threads are inactive waiting for memory access.    
On the left we show that explicit dependencies between instructions in the pipeline are reintroduced.
However, by inserting pipeline stalls to meet the minimum required thread count, the dependencies are once again removed.
This is shown on the right.   
Employing more total threads in the pipeline can reduce the amount of stalling needed, since there is a larger pool of threads to select from.  
However, to ensure explicit dependencies are removed, stalls are \emph{always} required when the active thread count drops below the minimum.

More importantly however, the dynamic activation and deactivation of threads breaks temporal isolation between the threads.   
When a thread is deactivated, other threads are fetched more frequently into the pipeline.
At any one moment, the execution frequency of threads would depend on the number of active threads. 
Because a thread can deactivate based upon its own execution and affect other threads' execution frequency, threads are no longer temporally isolated.

In order to maintain temporal isolation between the threads, threads cannot affect the execution time of others.
For a time-predictable thread-interleaved pipeline, threads are not dynamically deactivated.
Instead, when a thread is fetched in the presence of a data hazard, a pipeline delay is inserted to preserve the round robin thread schedule.
This only slightly reduces the utilization of the pipeline, as other threads still executing during the long latency operation.
But the temporal isolation of threads is preserved, as the execution frequency of threads remain the same regardless of any thread activity.  
Compared to single threaded pipelines, the benefits of latency hiding from mulithreading are still present.

%The next instruction from thread 3 that is fetched into the pipeline is again the same \emph{ld} instruction.  
%As memory completes its execution during the execution of instructions from other threads, we replay the same instruction to pick up the results from memory and write it into registers to complete the execution of the \emph{ld} instruction. 
%It is possible to directly write the results back into the register file when the memory operation completes, without cycling the same instruction to pick up the results.
%This would require hardware additions to support and manage multiple write-back paths in the pipeline, and a multi write ported register file, so contention can be avoided with the existing executing threads.
%In our design we simply replay the instruction for write-backs to simplify design and piggy back on the existing write-back datapath.

Because no \emph{explicit} dependency exists between the instructions in the pipeline, the forwarding logic used to handle data hazards can be stripped out in thread interleaved pipelines. 
Data forwarding logic contains no internal state, so threads are temporally isolated even if it is present.   
However, the pipeline datapath can be greatly simplified in the absence of forwarding logic and branch predictors.
The static thread schedule reduces the overhead of context switches to almost none; it can be implemented with a simple $log(n)$ bit up-counter, where $n$ is the number of threads.     
This enables thread-interleaved pipelines to be clocked at faster clock speeds, because less logic exists between each pipeline stage.
  
\subsubsection{Structural Hazards}
Threads on a multithreaded architecture, by definition, share the underlying pipeline datapath and any hardware unit implemented in it.
Thus, multithreaded architectures are more susceptible to structural hazards, which can break temporal isolation if not handled predictably.

In multithreaded pipelines with a width of one, shared single-cycle hardware units do not cause structural hazards, because no contention arises from the pipelined instruction access.
However, multi-cycle hardware units cause structural hazards when consecutive instructions access the same unit. 
The second instruction needs to wait for the first to complete before obtaining access.
For thread-interleaved pipelines, this causes timing interference between threads, because consecutive instruction fetches come from different threads.
One thread's access to a multi-cycle hardware unit can cause another to delay.

If it is possible to pipeline the multi-cycle hardware unit to be single-cycle accessible, the structural hazard and timing interference can be eliminated.   
In our time-predictable thread interleaved pipeline, floating point hardware units are pipelined to be single-cycle accessible. 
Hence, they are shared predictably between the hardware threads, and cause no timing interference. 

If pipelining is not possible, then the management of contention for the hardware unit becomes essential to achieve temporal isolation of threads.     
The single memory unit in a thread-interleaved pipeline is an example of a shared, multi-cycle, non-pipeline-able hardware unit.
In this situation, a time division multiplex access (TDMA) schedule can be enforced to remove timing interference.
The TDMA schedule divides the access channel to the hardware unit into multiple time slots.  
Each thread only has access to the hardware unit at its assigned time slots, even if no other thread is currently accessing the unit.
By doing so, the access latency to the hardware unit is determined only by the timing offset between the thread and its access slot, not the activities of the other threads.
In section~\ref{section:memory_system} we show a predictable DRAM memory controller that use TDMA in the backend to schedule accesses to DRAM memory. 

It is important to understand that a TDMA schedule removes timing interference, but does \emph{not} remove structural hazards.
In fact, a TDMA schedule can further uncover the performance penalty of structural hazards.
By reserving privatized time slots for threads, the hardware unit will appear to be busy even when no thread is accessing it.
Thus, structural hazards can occur even when the hardware unit is not being used.
Although a TDMA schedule increases the average latency to access the hardware unit, the worst-case access latency is similar that of a conventional first-come-first-serve (FCFS) queuing based access schedule with a queue size of one.
In both cases, the worst-case access latency is needs to account for the accesses of all threads.      
However, by using a TDMA schedule to predictably handle the structural hazards, the temporal isolation of threads enable a much tighter and simpler WCET analysis~\cite{Lv:2010:CAI:1935940.1936246}.
%http://user.it.uu.se/~yi/pdf-files/RTSS10_65.pdf

% However, because the TDMA access schedule is static, and access time is decoupled from other threads, there is potential to obtain tighter timing analysis for accesses by inferring access slot hits and misses for future accesses. 
% For example, based upon the execution time offsets of a sequence of accesses to the shared resource, we may be able to conclude that at least one access will hit its TDMA access slot and get access right away.
% We can also possibly derive more accurate wait times for the accesses that do not hit its access slots based upon the elapsed time between accesses.
% An in depth study of WCET analysis of TDMA access schedules is beyond the scope of the thesis.
%But these are possibilities now because there is no timing interference between the threads. 
%A queue based mechanism would not be able to achieve better execution time analysis without taking into account the execution context of all other threads in the pipeline.

Even though shared single-cycle hardware units do not cause structural hazards, they can still introduce timing interference between threads in multithreaded architectures. 
Shared hardware units can create \emph{implicit} dependencies between threads if the internal hardware states can be updated by any thread. 
A shared branch predictor, as discussed earlier, is a prime example for this. 
Our thread-interleaved pipeline removes the need for a branch predictor by the interleaving of hardware threads.  
A shared cache is another example.   
A cache maintains internal state that determines if a memory access goes to the cache or to main memory.
There is typically an enormous latency difference between the two different accesses.
When the cache is shared between threads, the different interleaving of threads can affect the execution time of all threads. 
It is even possible to degrade the performance of the system if threads continuously evict cache lines from each other. 
This phenomenon is known as \emph{cache thrashing}.
Partitioned caches~\cite{cachepartition} in this case can be used to enforce separate internal states, so each thread updates only its own internal state.  
Our time-predictable thread-interleaved pipeline employs scratchpads instead of caches.
We discuss this in the context of a timing predictable memory hierarchy in section~\ref{section:memory_system}.   

As a side note, the sharing of internal hardware states between threads also increases security risks in multithreaded architectures. 
Side-channel attacks on encryption algorithms~\cite{Kelsey98sidechannel} exploit the shared hardware states to disrupt and probe the execution time of threads running the encryption algorithm.
The timing information can be used to crack the encryption key.
We show in section~\ref{sec:app_side_channel_attack} how our predictable thread-interleaved pipeline prevents timing side-channel attacks for encryption algorithms.   

\subsubsection{Deterministic Execution}
The time-predictable thread-interleaved pipeline uses multithreading to improve instruction throughput, and maintains temporal isolation of threads to achieve deterministic execution.  
To highlight these features, we show the isolated execution of threads within a thread-interleaved pipeline.
We use the example shown earlier (in figure~\ref{fig:execution_thread_interleaved_pipeline}), where we execute the sample GCD (figure ~\ref{fig:sample_gcd_code}) and data-dependent (figure ~\ref{fig:sample_data_dependent_code}) code on a 5 thread 5 stage thread-interleaved pipeline.   
Figure~\ref{fig:thread_isolated_execution} shows the execution of the first two threads in isolation. 
Thread 0 executes GCD, and thread 1 executes the data-dependent code.

\begin{figure}[h]
\begin{center}
\noindent\makebox[\textwidth]{%
\includegraphics[scale=.58]{figs/thread_isolated_execution}}
\end{center}
\vspace{-3mm}
\caption{Isolated execution of threads with a thread-interleaved pipeline}
\label{fig:thread_isolated_execution}
\end{figure}

From the perspective of a thread, most instructions observe a 5 cycle latency, as shown in figure ~\ref{fig:thread_isolated_execution}. 
The minimum observable latency for instructions depend on the number of threads in the pipeline. 
This can also be understood as the latency for each thread between instruction fetches.
In our time-predictable thread-interleaved pipeline, the static round robin thread schedule enables this latency to be constant.  
We use the term \emph{thread cycle} to encapsulate this latency, and simplify the numbers for timing analysis. 
In our example, the instructions shown in thread 0 each take 1 thread cycle.

The \emph{ldr} instruction in thread 1 accesses main memory.
From the thread-interleaving, the access latency to main memory is hidden the concurrent execution of other threads. 
Thus, long latency instructions can appear to have a reduced latency in the isolated view of threads.
In this example, the \emph{ldr} instruction observes only a 2 thread cycle latency, even though the actual memory access latency could have been up to 10 processor cycles. 

Threads are temporally isolated in our thread-interleaved pipeline, so execution of each thread can be analyzed in isolation.
From the isolated view of each thread, each instruction completes its execution before the next one is fetched, and no instruction is executed speculatively.
Because instructions do not overlap in execution, instruction's execution time is not affected by prior instructions.
Control hazards are eliminated because a branch or exception is resolved before the next instruction fetch. 
The long latencies caused by structural or data hazards are hidden from the thread interleaving, improving the throughput of the pipeline.    
We will describe in detail our implementation of the thread-interleaved pipeline in the beginning of chapter ~\ref{chapter:ptarm}.

\section{Memory System}
\label{section:memory_system}
\input{chapters/mem_sys}

\section{Instruction Set Architecture Extensions}
\label{sec:programming_models}
\input{chapters/prog_model}
