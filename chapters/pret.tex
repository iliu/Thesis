In this chapter we present the design principles of a PREcision Timed (PRET) Machine.
Specifically, we discuss the implementation of a predictable pipeline and memory controller, and present timing extensions to the ISA. 
It is important to understand why and how current architectures fall short of timing predictability and repeatability.
Thus, we first discuss the common architectural designs and their effects on execution time, and point out some key issues and trade-offs when designing architectures for predictable and repeatable timing.

\section{Pipelines}
The introduction of pipelining vastly improved the performance of processors.
Pipelining increases the number of instructions that can be processed at one time by splitting up instruction execution into multiple steps.
It allows for faster clock speeds, and improves instruction throughput compared to single cycle architectures.
Ideally each in processor cycle, one instruction completes and leaves the pipeline as another enters and begins execution. 
In reality, different pipeline hazards occur which reduce the throughput and create stalls in the pipeline.
The techniques introduced to mitigate the penalties of pipeline hazards greatly effect to the timing predictability and repeatability of architectures.     
We analyze several commonly used techniques to reduce the performance penalty from hazards, and show their effects on execution time and predictability. 

\subsection{Pipeline Hazards}
\label{sec:pipeline_hazards}
\subsubsection{Data Hazards}

\begin{wrapfigure}{r}{0.5\textwidth}
  \vspace{-30pt}
  \begin{center}
    \includegraphics[scale=.65]{figs/sample_data_dependent_code}
  \end{center}
  \vspace{-3mm}
  \caption{Sample code with data dependencies}
  \label{fig:sample_data_dependent_code}
\end{wrapfigure}

Data hazards occur when the data needed by an instruction is not yet available.
Pipelines begin the execution of instructions before preceding ones are finished, so consecutive instructions that are data-dependent could simultaneously be executing in the pipeline.
For example, the code in figure~\ref{fig:sample_data_dependent_code} shows assembly instructions from the ARM instruction set architecture (ISA) that each depend on the result of its previous instruction.
Figure~\ref{fig:data_depend_execution_non_interleaved} shows two ways data hazards are commonly handled in pipelines. 

\begin{figure}
\vspace{-20pt} 
\begin{center}
\includegraphics[scale=.6]{figs/data_depend_execution_non_interleaved}
\end{center}
\vspace{-3mm}
\caption{Handling of data dependencies in single threaded pipelines}
\label{fig:data_depend_execution_non_interleaved}
\end{figure}

In the figure, time progresses horizontally towards the right.
Each time step, or column, represents a processor cycle.
Each row represents an instruction that is fetched and executed within the pipeline.
Each block represents the instruction entering the different stages of the pipeline -- fetch (F), decode (D), execute (E), memory (M) and writeback (W).   
We assume a classic five stage RISC pipeline.

A simple but effective technique stalls the pipeline until the previous instruction completes.
This is shown in the top of figure~\ref{fig:data_depend_execution_non_interleaved}, as delays are inserted to wait for the results from previous instructions.
The dependencies between instructions are explicitly shown in the figure to make clear why the pipeline delays are necessary.
The performance penalty incurred in this case comes from the pipeline delays inserted.

\emph{Data forwarding} is commonly used to mitigate the need for inserting delays when data hazards occur.
Pipelines split up the execution of instructions into different execution stages. 
Thus, the results from an instruction could be ready, but waiting to be committed in the last stage of the pipeline.
Data forwarding utilizes this and introduces backwards data paths in the pipeline, so earlier pipeline stages can access the data from instructions in later stages that have not yet committed.
This greatly reduces the amount of delays inserted in the pipeline, as instructions can access the results of previous instructions before they commit.
The circuitry of data forwarding usually consists of the backwards data paths and multiplexers in the earlier pipeline stages to select the correct data to be used.    
The pipeline controller dynamically detects whether a data-dependency exists, and changes the selection bits of the multiplexers accordingly to select the correct operands.

The bottom of figure~\ref{fig:data_depend_execution_non_interleaved} shows the execution sequence of the previous example in a pipeline with data forwarding.
No pipeline delays are inserted for the first \emph{sub} and \emph{ldr} instruction because the data they depend on are forwarded with the forwarding paths.
However, delays are still inserted for the second \emph{sub} instruction after the \emph{ld} instruction.
For longer latency operations, such as memory accesses, the results are not yet available to be forwarded by the forwarding paths, so pipeline delays are still required. 
This illustrates the limitations of data forwarding.
They can address data hazards that result from pipelining, such as read-after-write register operations, but they cannot address data hazards that result from long latency operations, such as memory operations.
More involved techniques such as the out-of-order execution or superscalars are required to mitigate the effects of long latency operations.

The handling of data hazards in pipelines can cause instructions to exhibit dynamic execution times.  
For example, figure~\ref{fig:data_depend_execution_non_interleaved} shows the \emph{sub} instruction, in both top and bottom figures, exhibiting different execution times. 
To determine the execution time of instructions on pipelines that stall for data hazards, we need to determine when a stall is inserted, and how long the pipeline is stalled for.
Stalls are required when the current instruction uses the results of a previous instruction that is still in execution in the pipeline.
Thus, depending on the pipeline depth, a window of previous instructions needs to be checked to determine if any stalls are inserted.     
The length of the stall is determined by the execution time of the dependent instruction, because the pipeline will stall until that instruction completes.
Data forwarding does not remove the data hazards, but only reduces the number of stalls required to take care of the data hazards.  
Thus, to determine the execution time when data forwarding is used, timing analysis needs to determine when the data forwarding circuitry cannot not forward the data for data hazards.
This can similarly be done by observing the window of instructions not yet committed in the pipeline.  
The difference is, instead of all data dependencies, only data dependencies from long latency operations need to be detected.

Both techniques used for handling data hazards caused the execution time of instructions to depend on a window of previous instructions.
The deeper the pipeline, the larger the window of instructions execution time will depend on. 
Thus, static execution time analysis needs to model and account for this additional the window of instructions on pipelined architectures that use stalling or forwarding to handle the data hazards. 

\subsubsection{Control Hazards}
\begin{wrapfigure}{r}{0.5\textwidth}
  \vspace{-20pt}
  \begin{center}
    \includegraphics[scale=.65]{figs/sample_gcd_code}
  \end{center}
  \vspace{-3mm}
  \caption{GCD with conditional branches}
  \label{fig:sample_gcd_code}
\end{wrapfigure}

Branches are the most common cause of control-flow hazards (or control hazards) in the pipeline; the instruction after the branch, which should be fetched the next cycle, is unknown until after the branch instruction is completed.
Conditional branches further complicates matters, as whether or not the branch is taken depends on an additional condition that could also be unknown when the conditional branch is in execution. 
The code segment in figure~\ref{fig:sample_gcd_code} implements the \emph{Greatest Common Divisor} (GCD) algorithm using the conditional branch instructions \emph{beq} (branch equal) and \emph{blt} (branch less than) in the ARM ISA.  
Conditional branch instructions in ARM branch based on conditional bits that are stored in a processor state register.
Those conditional bits can be set based on the results of standard arithmetic instructions \todo{cite arm manual}.
The \emph{cmp} instruction is one such instruction that subtracts two registers and sets the conditional bits according to the results.
The GCD implementation shown in the code uses this mechanism to determine whether to continue or end the algorithm.
Figure~\ref{fig:branch_execution_non_interleaved_pipeline} shows the execution of the conditional branches from our example, and demonstrates two commonly used techniques to handling control hazards in pipelines. 
To show only the timing effects of handling control hazards, we assume an architecture with data forwarding that handles data hazards.
As there are no long latency instructions in our example, all stalls observed in the figure are caused by the handling of control hazards.  

\begin{figure}
\begin{center}
\noindent\makebox[\textwidth]{%
\includegraphics[scale=.58]{figs/branch_execution_non_interleaved_pipeline}}
\end{center}
\vspace{-3mm}
\caption{Handling of conditional branches in single threaded pipelines}
\label{fig:branch_execution_non_interleaved_pipeline}
\end{figure}

Similar to data hazards, control hazards can also be handled by stalling the pipeline until the branch instruction completes. 
This is shown on the left of figure~\ref{fig:branch_execution_non_interleaved_pipeline}. 
%The dependencies between instructions are explicitly shown to make clear why the pipeline delays are necessary.
Branch instructions typically calculate the target address in the execute stage, so two pipeline delays are inserted before the fetching of the \emph{blt} instruction, to wait for \emph{beq} to complete the target address calculation. 
The reasoning applies to the two pipeline delays inserted before the \emph{sub} instruction. 
The performance penalty (often referred to as the \emph{branch penalty}) incurred in this case is the two delays inserted after every branch instruction, to wait for the branch address calculation to complete.

To mitigate the branch penalty, some architectures enforce the compiler to insert one or more non-dependent instructions after each branch instruction.
These instruction slots are called branch delay slots, and are always executed before the pipeline branches to the target address. 
This way, instead of wasting cycles to wait for the target address calculation, the pipeline continues to execute useful instructions before it branches.
However, if the compiler cannot place useful instructions in the branch delay slot, \emph{nops} need to be inserted into those slots to ensure correct program execution.
Thus, branch delay slots are less effective for deeper pipelines because more delay slots need to be filled by the compiler to account for the branch penalty.
  
Instead of stalling, \emph{branch predictors} are commonly employed to predict the branch condition and target address so the pipeline can speculatively continue execution. 
Branch predictors internally maintain some form of state machine that is used to determine the prediction of each branch.  
The internal state is updated after each branch according to the results of the branch. 
Different prediction schemes have been proposed, and some can even accurately predict branches up to 93.5\%\todo{citation}.  
If the branch prediction is correct, no penalty is incurred for the branch because the correct instructions were speculatively executed.  
%With branch predictor, the pipeline fetches the next instruction based upon the results of the branch prediction, and continues to execute speculatively.
However, when the prediction is incorrect (often referred as a \emph{branch midpredict}), the speculatively executed instructions are flushed, and the correct instructions are re-fetched into the pipeline for execution.

The right of figure~\ref{fig:branch_execution_non_interleaved_pipeline} shows the execution of GCD in the case of a branch misprediction.
The \emph{beq} branch is predicted to be taken, so the \emph{add} and \emph{mov} instructions from label \emph{end} are directly fetched into execution. 
When \emph{beq} progresses past the execute stage, \emph{cmp} has forwarded its results used to determine the branch condition, and the branch target address has been calculated, so the branch is resolved.
At this point, the misprediction is detected, so the \emph{add} and \emph{mov} instruction are flushed out of the pipeline. 
The next instruction from the correct path, the \emph{blt} instruction, is immediately re-fetched, and execution continues.
The performance penalty of branch mispredictions is derived from the number of pipeline stages between instruction fetch and branch resolution.  
In our example, the misprediction penalty is 2, as branches are resolved after the execute stage.
This penalty only occurs on a branch mispredict, thus branch predictors with high success rates typically improve average performance of pipelines drastically, compared to architectures that simply stall for branches.
%However, for more complex architectures with caches or other hardware states, the effects of incorrectly fetched instructions on the state of the processor less well-known and studied. 

The two methods of handling control hazards exhibit vastly different effects on execution time.    
When stalls are used to handle control hazards, the execution time effects are static and predictable.   
The pipeline will simply \emph{always} insert pipeline delays after a branch instruction.
Thus, no extra complexity is added to the execution time analysis; the latency of branch instructions simply need to be adjusted to include the branch penalty.
On the other hand, if a branch predictor is employed, the execution time of each branch will vary depending on the result of the branch prediction.  
To determine the success of a branch prediction, both the prediction and the branch outcome, both of which can dynamically change in run-time, must be known.   
Program path analysis can attempt to analyze the actual outcome of branches statically from the program code. 
The predictions made from the branch predictor depend on the internal state stored in the hardware unit.
This internal state, updated by each branch instruction, must be explicitly modeled in order to estimate the prediction. 
If the predictor state is unknown, the miss penalty must conservatively be accounted for.
There has been work on explicitly modeling branch predictors for execution time analysis\todo{citation}, but the results are \todo{the results of branch predictor modeling for execution time analysis}.
To make matters worse, the speculative execution on the predicted program paths lead to further complications that need to be accounted for.
Other internal states exist in the architecture that could be affected by speculatively executing instructions.
For example, if caches were used, their internal state could be updated during speculative execution of a mispredicted path.
As architectures grow in complexity, the combined modeling of all hardware states in the architecture often lead to an infeasible explosion in state space for the analysis.    
This makes a tight static execution time analysis extremely difficult, if not impossible.

The difference in execution time effects between stalling and employing a branch predictor highlight an important trade-off for architecture designs.    
It is possible to improve average-case performance by making predictions, and speculatively executing based upon them.
However, this comes at the cost of predictability, and a potential prolonging of the worst-case performance.  
For real-time and safety critical systems, the challenge remains to improve worst-case performance while maintaining predictability.
How pipeline hazards are handled play an integral part of tackling this challenge.           
 
Although less often mentioned, the presence of interrupts and exceptions in the pipeline also create control hazards. 
Exceptions can occur during the execution of any instruction, and changes the control flow of the program to execute the exception handler.
For single threaded pipelines, this means that all instructions fetched and not committed in the pipeline are speculative, because when an exception occurs, all uncommitted instructions in the pipeline become invalid.
Pipelines often handle this by flushing all instructions and fetching the exception handler for execution.      
These effects are acknowledged, but often ignored in static analysis because it is simply impossible to model every possible exception, and its effect on the architecture states. 
 
\subsubsection{Structural Hazards}
\emph{Structural hazards} occur when a processor's hardware component is needed by two or more instructions at the same time. 
For example, a single memory unit accessed both in the fetch and memory stage results in a structural hazard. 
The design of the pipeline plays an integral part in eliminating structural hazards. 
For example, the classic RISC five stage pipeline only issues one instruction at a time, and uses separate instruction and data caches to avoid structural hazards.
Structural hazards are generally much more prevalent in architectures that issue multiple instructions at a time.
If structural hazards cannot be avoided, then the pipeline must stall to access the contended hardware component sequentially.
The execution time effects of structural hazards are specific to how contention is managed for each pipeline design.
Here we omit a general discussion of the timing effects, and later address them specifically for our proposed architecture. 

\subsection{Pipeline Multithreading}
Discussed above, \emph{data forwarding} and \emph{branch prediction} are simple techniques employed to handle pipeline hazards. 
Advanced architectures, such as \emph{superscalar} and \emph{VLIW} machines, employ more complex mechanisms to improve the average performance of the architecture.  
Both architectures issue multiple instructions every cycle, and superscalar machines dynamically execute instructions out-of-order if no dependency is detected.    
These architectures exploit \emph{instruction-level parallelism} to overlap the execution of instructions from a single thread whenever possible.         
On the contrary, \emph{multithreaded architectures} exploit \emph{thread-level parallelism} to overlap the execution of instructions from different hardware threads. 
Each hardware thread in a multithreaded architecture has its own physical copy of a processor state, such as the registers file and program counter etc.
When a pipeline hazard arises from the execution of a hardware thread, another hardware thread can be fetched for execution to avoid stalling the pipeline. 
This improves the instruction throughput of the architecture.

\begin{figure}[h]
\begin{center}
\includegraphics[scale=.8]{figs/multithreaded_pipeline_block}
\end{center}
\vspace{-10pt}
\caption{Simple Multithreaded Pipeline}
\label{fig:multi-thread pipeline simplified}
\end{figure}
Figure~\ref{fig:multi-thread pipeline simplified} shows the implementation of a simple multithreaded pipeline.
It contains 5 hardware threads, so it has 5 copies of the Program Counter (PC) and Register files.
The rest of the pipeline remains similar to a classic five stage RISC pipeline, with the addition of a few multiplexers used to select the thread states.
Thus, the extra copies of the processor state and multiplexers are most of the hardware additions needed to implement hardware multithreading.
When a hardware thread executes in the pipeline, its corresponding thread state is passed into the pipeline to be used.
In most of this thesis, the term \emph{threads} to refer to the explicit hardware threads that have hardware copies of the thread state.
This is not to be confused with the common notion of \emph{threads}, which describes software contexts managed by an operating system, with its states stored in memory.
It will be explicitly noted when we refer to the software notion of threads. 
%The selection of threads for execution is one of the most important factors to fully utilize thread-level parallelism.
%If a thread is stalled waiting for memory access but gets selected to execute in the pipeline, then that instruction slot is wasted and the processor isn't fully utilized.
Ungerer et al.~\cite{Ungerer:2003:survey_multithreading} surveyed different multithreaded architectures and categorized them based upon the \emph{thread scheduling} policy and the \emph{execution width} of the pipeline.

The \emph{thread scheduling} policy determines which threads are executing, and how often a context switch occurs.  
\emph{Coarse-grain} policies manage threads similar to the way operation systems manage software threads.
A thread gains access to the pipeline and continues to execute until a context switch is triggered.
Context switches occur less frequently via this policy, so less threads are required to fully utilize the processor.
Different coarse-grain policies trigger context switches with different events. 
Some policies trigger context switches on dynamic events, such as a cache miss or an interrupt; some policies trigger context switches on more static events, such as specialized instructions.
\emph{Fine-grain} policies switch context much more frequently -- some as frequent as every processor cycle.
The \emph{execution width} of the pipeline is to the number of instructions fetched each cycle.  
Multithreaded architectures with wider pipeline widths can fetch all instructions a single thread, or mix instructions from different threads.
The Sumultanous Multithreaded (SMT) architecture~\todo{cite} is an example where instructions are fetched from different threads each cycle.

Multithreaded architectures present several challenges for static execution time analysis.
As figure~\ref{fig:multi-thread pipeline simplified} illustrated, threads share the hardware components within the pipeline.
If a hardware component, such as a branch predictor, maintains internal state, that internal state can be modified by all threads in the pipeline.
As the internal states of the hardware components affect the execution time of the individual instructions, each thread can affect the execution time of all threads in the pipeline. 
If the threads' execution time are interdependent, their timing cannot be separately analyzed.
As a result, in order to precisely model the hardware states, the execution order of instructions from all threads need to be known.
The interleaving of threads depend heavily on the thread scheduling policy, execution width, and hazard handling logic employed in the pipeline.
The compounding effect of these can create an overwhelming combination of possible thread interleavings, making static timing analysis nearly impossible, even if only a conservative estimation is desired.   

Nonetheless, we contend that thread-level parallelism (TLP) \emph{can} be exploited to handle pipeline hazards predictably. 
Even the most sophisticated architectures that fully exploit instruction-level parallelism (ILP) cannot guarantee enough parallelism in a single instruction stream to remove all stalls caused by pipeline hazards. 
This is known as the \emph{ILP Wall}~\todo{cite}. 
Conventional multithreaded architectures use coarse-grain thread scheduling policies to dynamically exploit TLP when there is not enough ILP to be exploited.    
However, the compounding effects of the combined architectural features lead to unpredictable architectural timing behaviors.
Instead, a \emph{thread-interleaved pipeline} fully exploits TLP with a fine-grained thread scheduling policy.
We show that with several predictable architectural adjustments to the thread-interleaved pipeline, we can achieve a fully time-predictable pipeline 
with deterministic execution time behaviors.     

\subsection{A Predictable Thread-Interleaved Pipeline}
\label{section:pret_thread_pipeline}
Thread-interleaved pipelines use a fine-grain thread scheduling policy; every cycle a different hardware thread is fetched for execution.
A round robin scheduling policy is often employed to reduce the context switch overhead every cycle.     
The thread-interleaved pipeline is known for implementing the peripheral processors of the Cray Multi-Threaded Architecture (MTA) multiprocessors~\todo{cite}.
Each the ``peripheral processor'' is implemented as a hardware thread.     
Interacting with input/output peripherals often lead to idle processor cycles that waits for the peripherals' responses.
By interleaving several threads, thread-level parallelism is fully exploited, and the idle cycles can be used for simultaneous interaction with multiple input/output devices.       
Figure~\ref{fig:execution_thread_interleaved_pipeline} shows an example execution sequence from a 5 stage single width thread-interleaved pipeline with 5 threads.
\begin{figure}[h]
    \begin{center}
    \includegraphics[scale=.55]{figs/thread-interleaved-execution}
  \end{center}
  \vspace{-10pt}
  \caption{Sample execution sequence of a thread-interleaved pipeline with 5 threads and 5 pipeline stages}
  \label{fig:execution_thread_interleaved_pipeline}
\end{figure}

The same code segments from figure~\ref{fig:sample_gcd_code} and figure~\ref{fig:sample_data_dependent_code} are used in this example. 
Threads 0, 2 and 4 execute GCD (figure~\ref{fig:sample_gcd_code}) and threads 1 and 3 execute the data dependent code segment (figure~\ref{fig:sample_data_dependent_code}).
The thick arrows on the left show the initial execution progresses of each thread at time 0.
We observe from the figure that each time step, an instruction from a different hardware thread is fetched in round robin order.
By time step 4, each pipeline stage is occupied by a different hardware thread.
The fine-grained thread interleaving and round robin scheduling combine to form this important property for thread-interleaved pipelines, which provides the basis for a timing predictable pipeline design.

The interleaving of threads by itself does not guarantee timing predictability for the pipeline.  
Shared hardware components or a selective thread execution policy can easily allow the execution time of threads to be affected by each other.    
As previously discussed, a combined timing analysis of all threads in the pipeline is extremely difficult, if not impossible.
In order for multithreaded architectures to achieve predictable performance, threads must be temporally isolated from one another. 
Temporal isolation removes cross-thread timing dependencies to allows timing analysis of threads independently.
This enables a simple and more precise execution time analysis.  
We refine several features on the thread-interleaved pipeline to temporally isolate the threads and predictably handle pipeline hazards.
This establishes a time-predictable thread-interleaved pipeline.

\subsubsection{Control Hazards}
By interleaving enough threads, control hazards can be completely removed from the pipeline.
This effect can be observed and explained from the execution sequence shown in figure~\ref{fig:execution_thread_interleaved_pipeline}.
At time 2, a \emph{blt} instruction from thread 2 is fetched into the pipeline.
On a single-threaded pipeline, stalls or branch prediction would be required to fetch the instruction at the next time step.
In this example, the next instruction fetched belongs to thread 3, and does not depend on the branch results of the \emph{blt} instruction.
No control hazard occurs from the branch, so no stall or branch prediction is needed to fetch the instruction.
The instruction fetch that depends on the branch result occurs at time 7.
Again, there is no control hazard since the branch is resolved by this point, so the instruction from the correct program path is fetched.    
Because control hazards from branches are eliminated, branch predictors are not needed in our pipeline design. 
Removing the branch predictor contributes to the temporal isolation of threads, as the shared internal state of the  branch predictor can create execution time dependencies between threads.  

The interleaving of threads also eliminate control hazards in the presence of exceptions.
If the pipeline detects an exception for the \emph{blt} instruction in the writeback stage (time 6), the control flow for thread 2 is changed to handle the exception.  
Because no other instruction in the pipeline belongs to thread 2 at time 6, no instruction needs to be flushed.  
This reveals an important property our timing predictable pipeline, that \emph{no instruction is speculatively executed}.
The next instruction fetch from thread 2 does not occur until time 7.
At that point, any control flow change, including one caused by an exception, is already known.
Therefore, the correct program path is always executed.

The minimum number of threads required to eliminate control hazards depend on the number of the pipeline stages.
Conservatively, employing the same number of threads as pipeline stages will always remove control hazards.
Intuitively, this is because at any point in time, each stage of the pipeline with be executing an instruction from a different hardware thread. 
Thus, no explicit dependency exists between instructions in the pipeline. 
However, Lee and Messerschmitt~\cite{lee1987pip} show that it is possible to use one less thread than the number of pipeline stages.
From here on, when we refer to the thread-interleaved pipeline, we assume enough threads to remove explicit dependencies between instructions in the pipeline.

\subsubsection{Data Hazards}
Because no explicit dependencies exist between instructions in the pipeline, data hazards that stem from the pipeline of instructions are removed. 
However, long latency operations can still cause data hazards in a thread-interleaved pipeline. 
This happens when a long latency operation is not completed before the next instruction fetch from the thread.
Although thread-interleaved pipelines can continue to fill the pipeline with other threads, data hazards are not completely eliminated.   
If all threads simultaneously execute a long latency operation, then no thread will be available to fill the pipeline. 

To maximizes pipeline utilization and instruction throughput, thread-interleaved pipelines can mark threads inactive when a long latency operation occurs.   
However, this dynamic behavior leads to non-trivial timing effects for the pipeline 
First, as stated earlier, a minimum number of threads is required to remove control hazards and the explicit dependencies between instructions in the pipeline. 
By allowing threads to be inactive, it is possible for the number of active threads to fall below this minimum. 
This can be circumvented by inserting pipeline stalls when the number active threads falls below the minimum.
This is illustrated in figure~\ref{fig:three_thread_pipeline}.
\begin{figure}[h]
  \vspace{-10pt}
  \begin{center}
    \includegraphics[scale=.6]{figs/three_thread_pipeline}
  \end{center}
  \vspace{-10pt}
  \caption{Execution of 5 threads thread-interleaved pipeline when 2 threads are inactive}
  \label{fig:three_thread_pipeline}
\end{figure}
This example shows the interleaving of 3 out of 5 threads in a 5 stage pipeline. 
We assume that 2 threads are inactive waiting for memory accesses.    
On the left we see that explicit dependency for instructions in the pipeline are reintroduced.
Only by inserting pipeline stalls to meet the minimum required thread count (shown on the right), can we assure that these dependencies are removed.   
The amount of stalling can be reduced if more threads are interleaved in the pipeline, since there is a larger pool of threads to select from. 
However, it is not possible to guarantee that stalls are never needed. 

More importantly, the dynamic activation and deactivation of threads breaks temporal isolation between the threads.   
When a thread is deactivated, other threads are fetched more frequently into the pipeline.
Thus, the execution frequency of threads would depend on number of active threads.
In order to maintain temporal isolation between the threads, our timing predictable thread-interleaved pipeline does not deactivate threads dynamically when long latency operations occur.

Instead, the thread scheduling policy of 

Although this slightly reduces the utilization of the thread-interleaved pipeline, but threads are decoupled and timing analysis can be done individually for each thread without interference from other threads.
At the same time, we still preserve most of the benefits of latency hiding, as other threads are still executing during the long latency operation.

%The next instruction from thread 3 that is fetched into the pipeline is again the same \emph{ld} instruction.  
%As memory completes its execution during the execution of instructions from other threads, we replay the same instruction to pick up the results from memory and write it into registers to complete the execution of the \emph{ld} instruction. 
%It is possible to directly write the results back into the register file when the memory operation completes, without cycling the same instruction to pick up the results.
%This would require hardware additions to support and manage multiple write-back paths in the pipeline, and a multi write ported register file, so contention can be avoided with the existing executing threads.
%In our design we simply replay the instruction for write-backs to simplify design and piggy back on the existing write-back datapath.

Because no explicit dependencies exist between instructions in the pipeline, data hazards that stem from the pipeline of instructions are removed. 
Thus, the forwarding logic normally used to handle data hazards are not needed in thread interleaved pipelines. 
Data forwarding logic contain no internal state, so threads can remain temporally isolated even if they are present.   
However, the pipeline design is greatly simplified in the absence of forwarding logic and branch predictors.   
This enables thread-interleaved pipelines to be clocked at faster clock speeds, because less logic exists between the pipeline stages.
  
\subsubsection{Structural Hazards}
Shared hardware units within multithreaded architectures could also easily break temporal isolation amongst the threads. 
Two main issues arise when a hardware unit is shared between the threads. 
The first issue arises when shared hardware units share the same state between all threads. 
If the state of hardware unit is shared and can be modified by any thread, then it is nearly impossible to get a consistent view of the hardware state from a single thread during timing analysis.
Shared branch predictors and caches are prime examples of how a shared hardware state can cause timing inference between threads.
If a multithreaded architecture shares a branch predictor for all threads, then the branch table entries can be overwritten by branches from any thread.
This means that each thread's branches can cause a branch mispredict for any other thread. 
Caches are especially troublesome when shared between threads in a multithreaded architecture. 
Not only does it make the execution time analysis substantially more difficult, it also decreases overall performance for each thread due to cache thrashing, an event where threads continuously evict each other threads cache lines in the cache\todo{citation}.
To achieve temporal isolation between the threads, the hardware units in the architecture must not share state between the threads. 
Each thread must have its own consistent view of the hardware unit states, without the interference from other threads.
For example, each thread in our thread-interleaved pipeline contains its own private copy of the registers and thread states.    
We already showed why thread-interleaved pipelines do not need branch predictors because they remove control-hazards, and we will discuss a timing predictable memory hierarchy that uses scratchpads instead of caches in section~\ref{section:memory_system}.
The sharing of hardware state between threads also increases security risks in multithreaded architectures. 
Side-channel attacks on encryption algorithms\todo{cite} take advantage of the shared hardware states to disrupt and probe the execution time of threads running the encryption algorithm to crack the encryption key.
We will discuss this in detail in section~\ref{sec:app_side_channel_attack} and show how a predictable architecture can prevent timing side-channel attacks for encryption algorithms.   

The second issue that arises is that shared hardware units create structural hazards -- hazards that occur when a hardware unit needs to be used by two or more instructions at the same time.
Structural hazards typically occur in thread-interleaved pipelines when the shared units take longer than one cycle to access. 
The ALU, for example, is shared between the threads.
But because it takes only one cycle to access, there is no contention even when instructions continuously access the ALU in subsequent cycles.
On the other hand, a floating point hardware unit typically takes several cycles to complete its computation.
If two or more threads issue a floating point instruction in subsequent cycles, then contention arises, and the the second request must be queued up until the first request completes its floating point computation.   
This creates timing interference between the threads, because the execution time of a floating point instruction from a particular thread now depends on if other threads are also issuing floating point instructions simultaneously. 
If the hardware unit can be pipelined to accept inputs every processor cycle, then we can remove the the contention caused by the hardware unit, since accesses no longer need to be queued up.
The shared memory system in a thread-interleaved pipeline also creates structural-hazards in the pipeline.
In section~\ref{section:memory_system} we will discuss and present our memory hierarchy along with a redesigned DRAM memory controller that supports pipelined memory accesses.
%The assumption here is still that we have a single issue pipeline architecture.   
If pipelining cannot be achieved, then any timing analysis of that instruction must include a conservative estimation that accounts for thread access interference and contention management.
Several trade-offs need to be considered when deciding how to manage the thread contention to the hardware unit.

A time division multiplex access (TDMA) schedule to the hardware unit can be enforced to decouple the access time of threads remove timing interference.
A TDMA access scheme certainly creates an non-substantial overhead compared to conventional queuing schemes, especially if access to the hardware unit is rare and sparse.
However, in a TDMA scheme, each threads wait time to access the shared resource depends on the time offset in regards to the TDMA schedule, and is decoupled from the accesses of other threads.
Because of that, it is possible obtain a tighter worst case execution time analysis per thread.
For a TDMA scheme, the worst case access time occurs when an access just missed its time slot and must wait a full cycle before accessing the hardware unit.
For a conventional queuing scheme where each requester can only have one outstanding request, the worst case happens when every other requester has a request in queue, and the first request is just beginning to be serviced.   
At first, it may seem that the worst case execution time of a TDMA scheme may seem similar to the basic queuing scheme.
For timing analysis at an unknown state of the program, no assumption can be made on the TDMA schedule, thus the worst case time must be used for conservative estimations. 
However, because the TDMA access schedule is static, and access time is decoupled from other threads, there is potential to obtain tighter timing analysis for accesses by inferring access slot hits and misses for future accesses. 
For example, based upon the execution time offsets of a sequence of accesses to the shared resource, we may be able to conclude that at least one access will hit its TDMA access slot and get access right away.
We can also possibly derive more accurate wait times for the accesses that do not hit its access slots based upon the elapsed time between accesses.
An in depth study of WCET analysis of TDMA access schedules is beyond the scope of the thesis.
But these are possibilities now because there is no timing interference between the threads. 
A queue based mechanism would not be able to achieve better execution time analysis without taking into account the execution context of all other threads in the pipeline.

\bigskip

\todo{Compare execution sequences of previous figures of threads. Highlight the simpler timing analysis}

\todo{highlight that the observable latency of long latency instructions are predictably reduced}
For operations that have long latencies, such as memory operations or floating point operations, thread-interleaved pipelines hides the latency with its execution of other threads. 
Thread 3 in figure~\ref{fig:execution_thread_interleaved_pipeline} shows the execution of a \emph{ld} instruction that takes the same 5 cycles as shown in figure~\ref{fig:data_depend_execution_non_interleaved}.
We again assume that this \emph{ld} instruction accesses data from the main memory. 
While the \emph{ld} instruction is waiting for memory access to complete, the thread-interleaved pipeline executes instructions from other threads.

It is important to understand that we are not proclaiming that all dynamic behavior in systems are harmful. 
But only by achieving predictability in the hardware architecture can we begin to reason about more dynamic behavior in software.
For example, we discussed that dynamically scheduling threads in hardware causes timing interference. 
However, it is not the switching of threads that is unpredictable, but how the thread switching is triggered that makes it predictable.   
For example, the Giotto\todo{cite} programming model specifies a periodic software execution model that can contain multiple program states. 
If such a programming model was implemented on a thread-interleaved pipeline, different program states might map different tasks to threads or have different number of threads executing within the pipeline.
But by explicitly controller the thread switches in software, the execution time variances introduced is transparent at the software level, allowing potential for timing analysis.

%TODO: Talk about the trade-offs of threaded interleaved pipelines to summarize 
In this section we introduced a predictable thread-interleaved pipeline design that provides temporal isolation for all threads in the architecture.
The thread-interleaved pipeline favors throughput over single thread latency, as multiple threads are executed on the pipeline in a round robin fashion.  
We will present in detail our implementation of this thread-interleaved pipeline in chapter~\ref{chapter:ptarm}, and show how the design decisions discussed in this chapter are applied.

\section{Memory System}
\label{section:memory_system}
\input{chapters/mem_sys}

\section{Instruction Set Architecture Extensions}
\label{sec:programming_models}
\input{chapters/prog_model}
